{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 경로 확인 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ADMIN\\\\Desktop\\\\dl업로드용\\\\Kobertsum_0.3v'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\Desktop\\dl업로드용\\Kobertsum_0.3v\\src\n"
     ]
    }
   ],
   "source": [
    "cd ./src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 필요 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertConfig\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from models.model_builder import *\n",
    "from models.encoder import *\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from prepro.tokenization_kobert import *\n",
    "from prepro.tokenization_kobert import KoBertTokenizer\n",
    "from kss import split_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 인자 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-encoder\", default='bert', type=str, choices=['bert', 'baseline'])\n",
    "parser.add_argument(\"-bert_data_path\", default='../bert_data_new/cnndm')\n",
    "parser.add_argument(\"-model_path\", default='../models/')\n",
    "parser.add_argument(\"-result_path\", default='../results/cnndm')\n",
    "parser.add_argument(\"-temp_dir\", default='../temp')\n",
    "\n",
    "parser.add_argument(\"-batch_size\", default=140, type=int)\n",
    "parser.add_argument(\"-test_batch_size\", default=200, type=int)\n",
    "\n",
    "parser.add_argument(\"-max_pos\", default=512, type=int)\n",
    "parser.add_argument(\"-use_interval\", type=str2bool, nargs='?',const=True,default=True)\n",
    "parser.add_argument(\"-large\", type=str2bool, nargs='?',const=True,default=False)\n",
    "parser.add_argument(\"-load_from_extractive\", default='', type=str)\n",
    "\n",
    "parser.add_argument(\"-sep_optim\", type=str2bool, nargs='?',const=True,default=False)\n",
    "parser.add_argument(\"-lr_bert\", default=2e-3, type=float)\n",
    "parser.add_argument(\"-lr_dec\", default=2e-3, type=float)\n",
    "parser.add_argument(\"-use_bert_emb\", type=str2bool, nargs='?',const=True,default=False)\n",
    "\n",
    "parser.add_argument(\"-share_emb\", type=str2bool, nargs='?', const=True, default=False)\n",
    "parser.add_argument(\"-finetune_bert\", type=str2bool, nargs='?', const=True, default=True)\n",
    "parser.add_argument(\"-dec_dropout\", default=0.2, type=float)\n",
    "parser.add_argument(\"-dec_layers\", default=6, type=int)\n",
    "parser.add_argument(\"-dec_hidden_size\", default=768, type=int)\n",
    "parser.add_argument(\"-dec_heads\", default=8, type=int)\n",
    "parser.add_argument(\"-dec_ff_size\", default=2048, type=int)\n",
    "parser.add_argument(\"-enc_hidden_size\", default=512, type=int)\n",
    "parser.add_argument(\"-enc_ff_size\", default=512, type=int)\n",
    "parser.add_argument(\"-enc_dropout\", default=0.2, type=float)\n",
    "parser.add_argument(\"-enc_layers\", default=6, type=int)\n",
    "\n",
    "parser.add_argument(\"-pretrained_model\", default='bert', type=str)\n",
    "\n",
    "parser.add_argument(\"-mode\", default='', type=str)\n",
    "parser.add_argument(\"-select_mode\", default='greedy', type=str)\n",
    "parser.add_argument(\"-map_path\", default='../../data/')\n",
    "parser.add_argument(\"-raw_path\", default='../../line_data')\n",
    "parser.add_argument(\"-save_path\", default='../../data/')\n",
    "\n",
    "parser.add_argument(\"-shard_size\", default=2000, type=int)\n",
    "parser.add_argument('-min_src_nsents', default=1, type=int)    # 3\n",
    "parser.add_argument('-max_src_nsents', default=120, type=int)    # 100\n",
    "parser.add_argument('-min_src_ntokens_per_sent', default=1, type=int)    # 5\n",
    "parser.add_argument('-max_src_ntokens_per_sent', default=300, type=int)    # 200\n",
    "parser.add_argument('-min_tgt_ntokens', default=1, type=int)    # 5\n",
    "parser.add_argument('-max_tgt_ntokens', default=500, type=int)    # 500\n",
    "\n",
    "parser.add_argument(\"-lower\", type=str2bool, nargs='?',const=True,default=True)\n",
    "parser.add_argument(\"-use_bert_basic_tokenizer\", type=str2bool, nargs='?',const=True,default=False)\n",
    "\n",
    "parser.add_argument('-log_file', default='../../logs/cnndm.log')\n",
    "\n",
    "parser.add_argument('-dataset', default='')\n",
    "\n",
    "parser.add_argument('-n_cpus', default=2, type=int)\n",
    "\n",
    "# params for EXT\n",
    "parser.add_argument(\"-ext_dropout\", default=0.2, type=float)\n",
    "parser.add_argument(\"-ext_layers\", default=2, type=int)\n",
    "parser.add_argument(\"-ext_hidden_size\", default=768, type=int)\n",
    "parser.add_argument(\"-ext_heads\", default=8, type=int)\n",
    "parser.add_argument(\"-ext_ff_size\", default=2048, type=int)\n",
    "\n",
    "parser.add_argument(\"-label_smoothing\", default=0.1, type=float)\n",
    "parser.add_argument(\"-generator_shard_size\", default=32, type=int)\n",
    "parser.add_argument(\"-alpha\",  default=0.6, type=float)\n",
    "parser.add_argument(\"-beam_size\", default=5, type=int)\n",
    "parser.add_argument(\"-min_length\", default=15, type=int)\n",
    "parser.add_argument(\"-max_length\", default=150, type=int)\n",
    "parser.add_argument(\"-max_tgt_len\", default=140, type=int)\n",
    "\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BertData 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertData():\n",
    "    def __init__(self):\n",
    "        #self.tokenizer = KoBertTokenizer.from_pretrained(\"monologg/kobert\", do_lower_case=True)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"monologg/kobigbird-bert-base\")\n",
    "\n",
    "        self.sep_token = '[SEP]'\n",
    "        self.cls_token = '[CLS]'\n",
    "        self.pad_token = '[PAD]'\n",
    "        # self.sep_vid = self.tokenizer.token2idx[self.sep_token]\n",
    "        # self.cls_vid = self.tokenizer.token2idx[self.cls_token]\n",
    "        # self.pad_vid = self.tokenizer.token2idx[self.pad_token]\n",
    "        self.sep_vid = self.tokenizer.convert_tokens_to_ids(self.sep_token)\n",
    "        self.cls_vid = self.tokenizer.convert_tokens_to_ids(self.cls_token)\n",
    "        self.pad_vid = self.tokenizer.convert_tokens_to_ids(self.pad_token)\n",
    "\n",
    "    def preprocess(self, src):\n",
    "\n",
    "        if (len(src) == 0):\n",
    "            return None\n",
    "\n",
    "        original_src_txt = [' '.join(s) for s in src]\n",
    "        idxs = [i for i, s in enumerate(src) if (len(s) > 1)]\n",
    "\n",
    "        src = [src[i][:2000] for i in idxs]\n",
    "        src = src[:1000]\n",
    "\n",
    "        if (len(src) < 3):\n",
    "            return None\n",
    "\n",
    "        src_txt = [' '.join(sent) for sent in src]\n",
    "        text = ' [SEP] [CLS] '.join(src_txt)\n",
    "        src_subtokens = self.tokenizer.tokenize(text)\n",
    "        #src_subtokens = src_subtokens[:4094]  ## 512가 최대인데 [SEP], [CLS] 2개 때문에 510\n",
    "        src_subtokens = ['[CLS]'] + src_subtokens + ['[SEP]']\n",
    "\n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
    "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "        segments_ids = []\n",
    "        for i, s in enumerate(segs):\n",
    "            if (i % 2 == 0):\n",
    "                segments_ids += s * [0]\n",
    "            else:\n",
    "                segments_ids += s * [1]\n",
    "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
    "        labels = None\n",
    "        src_txt = [original_src_txt[i] for i in idxs]\n",
    "        tgt_txt = None\n",
    "        \n",
    "        return src_subtoken_idxs, labels, segments_ids, cls_ids, src_txt, tgt_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout, dim, max_len=5000):\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp((torch.arange(0, dim, 2, dtype=torch.float) *\n",
    "                              -(math.log(10000.0) / dim)))\n",
    "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, emb, step=None):\n",
    "        emb = emb * math.sqrt(self.dim)\n",
    "        if (step):\n",
    "            emb = emb + self.pe[:, step][:, None, :]\n",
    "\n",
    "        else:\n",
    "            emb = emb + self.pe[:, :emb.size(1)]\n",
    "        emb = self.dropout(emb)\n",
    "        return emb\n",
    "\n",
    "    def get_emb(self, emb):\n",
    "        return self.pe[:, :emb.size(1)]\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadedAttention(\n",
    "            heads, d_model, dropout=dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, iter, query, inputs, mask):\n",
    "        if (iter != 0):\n",
    "            input_norm = self.layer_norm(inputs)\n",
    "        else:\n",
    "            input_norm = inputs\n",
    "\n",
    "        mask = mask.unsqueeze(1)\n",
    "        context = self.self_attn(input_norm, input_norm, input_norm,\n",
    "                                 mask=mask)\n",
    "        out = self.dropout(context) + inputs\n",
    "        return self.feed_forward(out)\n",
    "\n",
    "class ExtTransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, heads, dropout, num_inter_layers=0):\n",
    "        super(ExtTransformerEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_inter_layers = num_inter_layers\n",
    "        self.pos_emb = PositionalEncoding(dropout, d_model)\n",
    "        self.transformer_inter = nn.ModuleList(\n",
    "            [TransformerEncoderLayer(d_model, heads, d_ff, dropout)\n",
    "             for _ in range(num_inter_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.wo = nn.Linear(d_model, 1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, top_vecs, mask):\n",
    "        \"\"\" See :obj:`EncoderBase.forward()`\"\"\"\n",
    "\n",
    "        batch_size, n_sents = top_vecs.size(0), top_vecs.size(1)\n",
    "        pos_emb = self.pos_emb.pe[:, :n_sents]\n",
    "        x = top_vecs * mask[:, :, None].float()\n",
    "        x = x + pos_emb\n",
    "\n",
    "        for i in range(self.num_inter_layers):\n",
    "            x = self.transformer_inter[i](i, x, x, ~ mask)  # all_sents * max_tokens * dim\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        sent_scores = self.sigmoid(self.wo(x))\n",
    "        sent_scores = sent_scores.squeeze(-1) * mask.float()\n",
    "\n",
    "        return sent_scores\n",
    "    \n",
    "# class Bert(nn.Module):\n",
    "#     temp_dir = 'D:/KoBertSum/temp'\n",
    "#     def __init__(self, large, temp_dir, finetune=False):\n",
    "#         super(Bert, self).__init__()\n",
    "#         self.model = BertModel.from_pretrained(\"monologg/kobert\", cache_dir=temp_dir)\n",
    "\n",
    "#         self.finetune = finetune\n",
    "\n",
    "#     def forward(self, x, segs, mask):\n",
    "#         top_vec = self.model(input_ids=x, token_type_ids=segs, attention_mask=mask)[0]\n",
    "\n",
    "#         return top_vec\n",
    "    \n",
    "class BigBird(nn.Module):\n",
    "    def __init__(self, temp_dir, finetune=False):\n",
    "        super(BigBird, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(\"monologg/kobigbird-bert-base\") # 모델 변경\n",
    "        self.finetune = finetune\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        if self.finetune:\n",
    "            outputs = self.model(x, attention_mask=mask)\n",
    "            top_vec = outputs.last_hidden_state\n",
    "        else:\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(x, attention_mask=mask)\n",
    "                top_vec = outputs.last_hidden_state\n",
    "        return top_vec \n",
    "    \n",
    "class ExtSummarizer(nn.Module):\n",
    "    def __init__(self, args, device, checkpoint):\n",
    "        super(ExtSummarizer, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        #self.bert = Bert(args.large, args.temp_dir)\n",
    "        self.roberta = AutoModel.from_pretrained(\"monologg/kobigbird-bert-base\")\n",
    "        #self.bert = BigBird(args.large, args.temp_dir)\n",
    "\n",
    "        # self.ext_layer = ExtTransformerEncoder(self.bert.model.config.hidden_size, args.ext_ff_size, args.ext_heads,\n",
    "        #                                        args.ext_dropout, args.ext_layers)\n",
    "        self.ext_layer = ExtTransformerEncoder(self.roberta.config.hidden_size, args.ext_ff_size, args.ext_heads,\n",
    "                                               args.ext_dropout, args.ext_layers)\n",
    "        # if (args.encoder == 'baseline'):\n",
    "        #     bert_config = BertConfig(self.bert.model.config.vocab_size, hidden_size=args.ext_hidden_size,\n",
    "        #                              num_hidden_layers=args.ext_layers, num_attention_heads=args.ext_heads, intermediate_size=args.ext_ff_size)\n",
    "        #     self.bert.model = BertModel(bert_config)\n",
    "        #     self.ext_layer = Classifier(self.bert.model.config.hidden_size)\n",
    "\n",
    "        if(args.max_pos>512):\n",
    "            my_pos_embeddings = nn.Embedding(args.max_pos, self.bert.model.config.hidden_size)\n",
    "            my_pos_embeddings.weight.data[:512] = self.bert.model.embeddings.position_embeddings.weight.data\n",
    "            my_pos_embeddings.weight.data[512:] = self.bert.model.embeddings.position_embeddings.weight.data[-1][None,:].repeat(args.max_pos-512,1)\n",
    "            self.bert.model.embeddings.position_embeddings = my_pos_embeddings\n",
    "\n",
    "\n",
    "        if checkpoint is not None:\n",
    "            self.load_state_dict(checkpoint['model'], strict=True)\n",
    "        else:\n",
    "            if args.param_init != 0.0:\n",
    "                for p in self.ext_layer.parameters():\n",
    "                    p.data.uniform_(-args.param_init, args.param_init)\n",
    "            if args.param_init_glorot:\n",
    "                for p in self.ext_layer.parameters():\n",
    "                    if p.dim() > 1:\n",
    "                        xavier_uniform_(p)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, src, segs, clss, mask_src, mask_cls):\n",
    "        top_vec = self.roberta(src)[0]\n",
    "        #print(top_vec)\n",
    "        #top_vec = top_vec.last_hidden_state\n",
    "        sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
    "        sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
    "        sent_scores = self.ext_layer(sents_vec, mask_cls).squeeze(-1)\n",
    "        return sent_scores, mask_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text):\n",
    "    \n",
    "    def txt2input(text):\n",
    "        #data = list(filter(None, text.split('\\n')))\n",
    "        #data = split_sentences(text)\n",
    "        bertdata = BertData()\n",
    "        txt_data = bertdata.preprocess(text)\n",
    "        data_dict = {\"src\":txt_data[0],\n",
    "                    \"labels\":[0,1,2],\n",
    "                    \"segs\":txt_data[2],\n",
    "                    \"clss\":txt_data[3],\n",
    "                    \"src_txt\":txt_data[4],\n",
    "                    \"tgt_txt\":None}\n",
    "        input_data = []\n",
    "        input_data.append(data_dict)\n",
    "        return input_data\n",
    "    \n",
    "    input_data = txt2input(text)\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    def _pad(data, pad_id, width=-1):\n",
    "        if (width == -1):\n",
    "            width = max(len(d) for d in data)\n",
    "        rtn_data = [d + [pad_id] * (width - len(d)) for d in data]\n",
    "        return rtn_data\n",
    "    \n",
    "    pre_src = [x['src'] for x in input_data]\n",
    "    pre_segs = [x['segs'] for x in input_data]\n",
    "    pre_clss = [x['clss'] for x in input_data]\n",
    "\n",
    "    src = torch.tensor(_pad(pre_src, 0)).cuda()\n",
    "    segs = torch.tensor(_pad(pre_segs, 0)).cuda()\n",
    "    mask_src = ~(src == 0)\n",
    "\n",
    "    clss = torch.tensor(_pad(pre_clss, -1)).cuda()\n",
    "    mask_cls = ~(clss == -1)\n",
    "    clss[clss == -1] = 0\n",
    "\n",
    "    clss.to(device).long()\n",
    "    mask_cls.to(device).long()\n",
    "    segs.to(device).long()\n",
    "    mask_src.to(device).long()\n",
    "    \n",
    "    # clss.to(device)\n",
    "    # mask_cls.to(device)\n",
    "    # segs.to(device)\n",
    "    # mask_src.to(device)\n",
    "    \n",
    "    #checkpoint = torch.load(\"D:/KoBertSum/ext/models/model_step_26000.pt\")  # V2\n",
    "    #checkpoint = torch.load(\"D:/KoBertSum/ext/models/model_step_5000.pt\")  # V1\n",
    "    #checkpoint = torch.load(\"D:/KoBertSum/ext/models/model_step_5000_2.pt\")  # V3 - max_pos 1024\n",
    "    checkpoint = torch.load(\"C:/Users/ADMIN/Desktop/dl업로드용/Kobertsum_0.3v/ext/models/1016_0350/model_step_27000.pt\")  # BigBird\n",
    "    model = ExtSummarizer(args, device, checkpoint)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sent_scores, mask = model(src, segs, clss, mask_src, mask_cls)\n",
    "        sent_scores = sent_scores + mask.float()\n",
    "        sent_scores = sent_scores.cpu().data.numpy()\n",
    "        print(sent_scores)\n",
    "        selected_ids = np.argsort(-sent_scores, 1)\n",
    "        print(selected_ids)\n",
    "    \n",
    "    return selected_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 길이 3485\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "서울 서초구 초등학교 교사 사망에 대한 교육당국 합동조사 결과가 지난 4일 나왔지만 새롭게 규명된 것이 없다는 교사들의 반발이 거세다. 교육부와 서울시교육청이 사건 직후 사실관계 확인에 나서겠다며 합동조사에 착수했지만, 대부분 의혹을 경찰 수사로 미뤄두면서 ‘용두사미 조사’가 되고 말았다는 것이다. 전국초등교사노조는 “‘새내기 교사의 죽음에 무거운 책임감을 느낀다’며 내놓은 결과라고 납득할 수 없을 정도로 허술하다. 합동조사단이 해야 할 일은 해당 학교가 낸 가정통신문 내용의 사실 확인이 아닌 고인의 업무상 고충을 면면히 공개하는 것이어야 했다”며 재조사를 촉구했다. 전국 교사 4만명은 지난 주말에도 철저한 진상규명을 촉구하는 집회를 이어갔다.\n",
    "실제로 합동조사 결과의 대부분은 학교 쪽이 고인의 사망 직후 냈던 입장문과 언론보도에서 제기된 내용이 맞는지 확인하는 수준에 그쳤다. 올해 3월 이후 고인의 학급 담임 교체 사실이 없었다거나, 해당 학급에서 올해 학교폭력 신고가 없었으며, 학급 내 이른바 ‘연필사건’ 이후 고인이 학부모로부터 전화를 받았다는 등의 단순 사실 확인에 불과하다. 연필사건 학생의 학부모가 고인의 휴대전화 번호를 알게 된 경위나 담임 자격 시비와 같은 폭언이 있었는지 여부, 학교 쪽이 연필사건을 원만히 중재했다고 한 7월13일 이후에도 추가적인 학부모 민원이 있었는지 등 정작 규명이 필요한 의혹에 대해서는 새롭게 밝혀낸 것이 하나도 없다.\n",
    "무엇보다 합동조사 결과에는 고인이 사망에 이르기까지 학교나 학교장의 책임은 없었는지에 대한 내용이 쏙 빠져 있다. 고인은 연필사건 학생뿐 아니라 다른 2명의 부적응 학생으로 인한 고충이 적지 않았고, 모두 10차례나 학교 쪽에 학생 지도의 어려움을 이유로 상담 요청을 한 바 있다. 그러나 학교 쪽은 고인에게 얼른 전화번호를 바꾸라거나 학부모에게 심리검사 또는 상담을 받을 것을 권유하라고 조언하는 정도에 그쳤다. 학교가 함께 문제를 해결하려는 의지를 보이는 대신 개별 교사에게 책임을 지운 정황이 아닐 수 없다. 심지어 학교 쪽은 지난달 최초 작성한 입장문에서 연필사건이 원만히 해결된 것처럼 언급했다가 해당 내용을 삭제했는데, 애초 어떤 의도로 작성한 것인지 규명될 필요가 있다. 교육당국은 재발 방지 대책을 촘촘하게 마련하는 일도 중요하지만 제대로 된 진상규명이 우선이라는 점을 명심해야 한다.\n",
    "서울 서초구 초등학교 교사 사망에 대한 교육당국 합동조사 결과가 지난 4일 나왔지만 새롭게 규명된 것이 없다는 교사들의 반발이 거세다. 교육부와 서울시교육청이 사건 직후 사실관계 확인에 나서겠다며 합동조사에 착수했지만, 대부분 의혹을 경찰 수사로 미뤄두면서 ‘용두사미 조사’가 되고 말았다는 것이다. 전국초등교사노조는 “‘새내기 교사의 죽음에 무거운 책임감을 느낀다’며 내놓은 결과라고 납득할 수 없을 정도로 허술하다. 합동조사단이 해야 할 일은 해당 학교가 낸 가정통신문 내용의 사실 확인이 아닌 고인의 업무상 고충을 면면히 공개하는 것이어야 했다”며 재조사를 촉구했다. 전국 교사 4만명은 지난 주말에도 철저한 진상규명을 촉구하는 집회를 이어갔다.\n",
    "실제로 합동조사 결과의 대부분은 학교 쪽이 고인의 사망 직후 냈던 입장문과 언론보도에서 제기된 내용이 맞는지 확인하는 수준에 그쳤다. 올해 3월 이후 고인의 학급 담임 교체 사실이 없었다거나, 해당 학급에서 올해 학교폭력 신고가 없었으며, 학급 내 이른바 ‘연필사건’ 이후 고인이 학부모로부터 전화를 받았다는 등의 단순 사실 확인에 불과하다. 연필사건 학생의 학부모가 고인의 휴대전화 번호를 알게 된 경위나 담임 자격 시비와 같은 폭언이 있었는지 여부, 학교 쪽이 연필사건을 원만히 중재했다고 한 7월13일 이후에도 추가적인 학부모 민원이 있었는지 등 정작 규명이 필요한 의혹에 대해서는 새롭게 밝혀낸 것이 하나도 없다.\n",
    "무엇보다 합동조사 결과에는 고인이 사망에 이르기까지 학교나 학교장의 책임은 없었는지에 대한 내용이 쏙 빠져 있다. 고인은 연필사건 학생뿐 아니라 다른 2명의 부적응 학생으로 인한 고충이 적지 않았고, 모두 10차례나 학교 쪽에 학생 지도의 어려움을 이유로 상담 요청을 한 바 있다. 그러나 학교 쪽은 고인에게 얼른 전화번호를 바꾸라거나 학부모에게 심리검사 또는 상담을 받을 것을 권유하라고 조언하는 정도에 그쳤다. 학교가 함께 문제를 해결하려는 의지를 보이는 대신 개별 교사에게 책임을 지운 정황이 아닐 수 없다. 심지어 학교 쪽은 지난달 최초 작성한 입장문에서 연필사건이 원만히 해결된 것처럼 언급했다가 해당 내용을 삭제했는데, 애초 어떤 의도로 작성한 것인지 규명될 필요가 있다. 교육당국은 재발 방지 대책을 촘촘하게 마련하는 일도 중요하지만 제대로 된 진상규명이 우선이라는 점을 명심해야 한다.\n",
    "서울 서초구 초등학교 교사 사망에 대한 교육당국 합동조사 결과가 지난 4일 나왔지만 새롭게 규명된 것이 없다는 교사들의 반발이 거세다. 교육부와 서울시교육청이 사건 직후 사실관계 확인에 나서겠다며 합동조사에 착수했지만, 대부분 의혹을 경찰 수사로 미뤄두면서 ‘용두사미 조사’가 되고 말았다는 것이다. 전국초등교사노조는 “‘새내기 교사의 죽음에 무거운 책임감을 느낀다’며 내놓은 결과라고 납득할 수 없을 정도로 허술하다. 합동조사단이 해야 할 일은 해당 학교가 낸 가정통신문 내용의 사실 확인이 아닌 고인의 업무상 고충을 면면히 공개하는 것이어야 했다”며 재조사를 촉구했다. 전국 교사 4만명은 지난 주말에도 철저한 진상규명을 촉구하는 집회를 이어갔다.\n",
    "실제로 합동조사 결과의 대부분은 학교 쪽이 고인의 사망 직후 냈던 입장문과 언론보도에서 제기된 내용이 맞는지 확인하는 수준에 그쳤다. 올해 3월 이후 고인의 학급 담임 교체 사실이 없었다거나, 해당 학급에서 올해 학교폭력 신고가 없었으며, 학급 내 이른바 ‘연필사건’ 이후 고인이 학부모로부터 전화를 받았다는 등의 단순 사실 확인에 불과하다. 연필사건 학생의 학부모가 고인의 휴대전화 번호를 알게 된 경위나 담임 자격 시비와 같은 폭언이 있었는지 여부, 학교 쪽이 연필사건을 원만히 중재했다고 한 7월13일 이후에도 추가적인 학부모 민원이 있었는지 등 정작 규명이 필요한 의혹에 대해서는 새롭게 밝혀낸 것이 하나도 없다.\n",
    "무엇보다 합동조사 결과에는 고인이 사망에 이르기까지 학교나 학교장의 책임은 없었는지에 대한 내용이 쏙 빠져 있다. 고인은 연필사건 학생뿐 아니라 다른 2명의 부적응 학생으로 인한 고충이 적지 않았고, 모두 10차례나 학교 쪽에 학생 지도의 어려움을 이유로 상담 요청을 한 바 있다. 그러나 학교 쪽은 고인에게 얼른 전화번호를 바꾸라거나 학부모에게 심리검사 또는 상담을 받을 것을 권유하라고 조언하는 정도에 그쳤다. 학교가 함께 문제를 해결하려는 의지를 보이는 대신 개별 교사에게 책임을 지운 정황이 아닐 수 없다. 심지어 학교 쪽은 지난달 최초 작성한 입장문에서 연필사건이 원만히 해결된 것처럼 언급했다가 해당 내용을 삭제했는데, 애초 어떤 의도로 작성한 것인지 규명될 필요가 있다. 교육당국은 재발 방지 대책을 촘촘하게 마련하는 일도 중요하지만 제대로 된 진상규명이 우선이라는 점을 명심해야 한다.\n",
    "\n",
    "'''\n",
    "print('입력 길이',len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['서울 서초구 초등학교 교사 사망에 대한 교육당국 합동조사 결과가 지난 4일 나왔지만 새롭게 규명된 것이 없다는 교사들의 반발이 거세다.',\n",
       " '교육부와 서울시교육청이 사건 직후 사실관계 확인에 나서겠다며 합동조사에 착수했지만, 대부분 의혹을 경찰 수사로 미뤄두면서 ‘용두사미 조사’가 되고 말았다는 것이다.',\n",
       " '전국초등교사노조는 “‘새내기 교사의 죽음에 무거운 책임감을 느낀다’며 내놓은 결과라고 납득할 수 없을 정도로 허술하다. 합동조사단이 해야 할 일은 해당 학교가 낸 가정통신문 내용의 사실 확인이 아닌 고인의 업무상 고충을 면면히 공개하는 것이어야 했다”며 재조사를 촉구했다.',\n",
       " '전국 교사 4만명은 지난 주말에도 철저한 진상규명을 촉구하는 집회를 이어갔다.',\n",
       " '실제로 합동조사 결과의 대부분은 학교 쪽이 고인의 사망 직후 냈던 입장문과 언론보도에서 제기된 내용이 맞는지 확인하는 수준에 그쳤다.',\n",
       " '올해 3월 이후 고인의 학급 담임 교체 사실이 없었다거나, 해당 학급에서 올해 학교폭력 신고가 없었으며, 학급 내 이른바 ‘연필사건’ 이후 고인이 학부모로부터 전화를 받았다는 등의 단순 사실 확인에 불과하다.',\n",
       " '연필사건 학생의 학부모가 고인의 휴대전화 번호를 알게 된 경위나 담임 자격 시비와 같은 폭언이 있었는지 여부, 학교 쪽이 연필사건을 원만히 중재했다고 한 7월13일 이후에도 추가적인 학부모 민원이 있었는지 등 정작 규명이 필요한 의혹에 대해서는 새롭게 밝혀낸 것이 하나도 없다.',\n",
       " '무엇보다 합동조사 결과에는 고인이 사망에 이르기까지 학교나 학교장의 책임은 없었는지에 대한 내용이 쏙 빠져 있다.',\n",
       " '고인은 연필사건 학생뿐 아니라 다른 2명의 부적응 학생으로 인한 고충이 적지 않았고, 모두 10차례나 학교 쪽에 학생 지도의 어려움을 이유로 상담 요청을 한 바 있다.',\n",
       " '그러나 학교 쪽은 고인에게 얼른 전화번호를 바꾸라거나 학부모에게 심리검사 또는 상담을 받을 것을 권유하라고 조언하는 정도에 그쳤다.',\n",
       " '학교가 함께 문제를 해결하려는 의지를 보이는 대신 개별 교사에게 책임을 지운 정황이 아닐 수 없다.',\n",
       " '심지어 학교 쪽은 지난달 최초 작성한 입장문에서 연필사건이 원만히 해결된 것처럼 언급했다가 해당 내용을 삭제했는데, 애초 어떤 의도로 작성한 것인지 규명될 필요가 있다.',\n",
       " '교육당국은 재발 방지 대책을 촘촘하게 마련하는 일도 중요하지만 제대로 된 진상규명이 우선이라는 점을 명심해야 한다.',\n",
       " '서울 서초구 초등학교 교사 사망에 대한 교육당국 합동조사 결과가 지난 4일 나왔지만 새롭게 규명된 것이 없다는 교사들의 반발이 거세다.',\n",
       " '교육부와 서울시교육청이 사건 직후 사실관계 확인에 나서겠다며 합동조사에 착수했지만, 대부분 의혹을 경찰 수사로 미뤄두면서 ‘용두사미 조사’가 되고 말았다는 것이다.',\n",
       " '전국초등교사노조는 “‘새내기 교사의 죽음에 무거운 책임감을 느낀다’며 내놓은 결과라고 납득할 수 없을 정도로 허술하다. 합동조사단이 해야 할 일은 해당 학교가 낸 가정통신문 내용의 사실 확인이 아닌 고인의 업무상 고충을 면면히 공개하는 것이어야 했다”며 재조사를 촉구했다.',\n",
       " '전국 교사 4만명은 지난 주말에도 철저한 진상규명을 촉구하는 집회를 이어갔다.',\n",
       " '실제로 합동조사 결과의 대부분은 학교 쪽이 고인의 사망 직후 냈던 입장문과 언론보도에서 제기된 내용이 맞는지 확인하는 수준에 그쳤다.',\n",
       " '올해 3월 이후 고인의 학급 담임 교체 사실이 없었다거나, 해당 학급에서 올해 학교폭력 신고가 없었으며, 학급 내 이른바 ‘연필사건’ 이후 고인이 학부모로부터 전화를 받았다는 등의 단순 사실 확인에 불과하다.',\n",
       " '연필사건 학생의 학부모가 고인의 휴대전화 번호를 알게 된 경위나 담임 자격 시비와 같은 폭언이 있었는지 여부, 학교 쪽이 연필사건을 원만히 중재했다고 한 7월13일 이후에도 추가적인 학부모 민원이 있었는지 등 정작 규명이 필요한 의혹에 대해서는 새롭게 밝혀낸 것이 하나도 없다.',\n",
       " '무엇보다 합동조사 결과에는 고인이 사망에 이르기까지 학교나 학교장의 책임은 없었는지에 대한 내용이 쏙 빠져 있다.',\n",
       " '고인은 연필사건 학생뿐 아니라 다른 2명의 부적응 학생으로 인한 고충이 적지 않았고, 모두 10차례나 학교 쪽에 학생 지도의 어려움을 이유로 상담 요청을 한 바 있다.',\n",
       " '그러나 학교 쪽은 고인에게 얼른 전화번호를 바꾸라거나 학부모에게 심리검사 또는 상담을 받을 것을 권유하라고 조언하는 정도에 그쳤다.',\n",
       " '학교가 함께 문제를 해결하려는 의지를 보이는 대신 개별 교사에게 책임을 지운 정황이 아닐 수 없다.',\n",
       " '심지어 학교 쪽은 지난달 최초 작성한 입장문에서 연필사건이 원만히 해결된 것처럼 언급했다가 해당 내용을 삭제했는데, 애초 어떤 의도로 작성한 것인지 규명될 필요가 있다.',\n",
       " '교육당국은 재발 방지 대책을 촘촘하게 마련하는 일도 중요하지만 제대로 된 진상규명이 우선이라는 점을 명심해야 한다.',\n",
       " '서울 서초구 초등학교 교사 사망에 대한 교육당국 합동조사 결과가 지난 4일 나왔지만 새롭게 규명된 것이 없다는 교사들의 반발이 거세다.',\n",
       " '교육부와 서울시교육청이 사건 직후 사실관계 확인에 나서겠다며 합동조사에 착수했지만, 대부분 의혹을 경찰 수사로 미뤄두면서 ‘용두사미 조사’가 되고 말았다는 것이다.',\n",
       " '전국초등교사노조는 “‘새내기 교사의 죽음에 무거운 책임감을 느낀다’며 내놓은 결과라고 납득할 수 없을 정도로 허술하다. 합동조사단이 해야 할 일은 해당 학교가 낸 가정통신문 내용의 사실 확인이 아닌 고인의 업무상 고충을 면면히 공개하는 것이어야 했다”며 재조사를 촉구했다.',\n",
       " '전국 교사 4만명은 지난 주말에도 철저한 진상규명을 촉구하는 집회를 이어갔다.',\n",
       " '실제로 합동조사 결과의 대부분은 학교 쪽이 고인의 사망 직후 냈던 입장문과 언론보도에서 제기된 내용이 맞는지 확인하는 수준에 그쳤다.',\n",
       " '올해 3월 이후 고인의 학급 담임 교체 사실이 없었다거나, 해당 학급에서 올해 학교폭력 신고가 없었으며, 학급 내 이른바 ‘연필사건’ 이후 고인이 학부모로부터 전화를 받았다는 등의 단순 사실 확인에 불과하다.',\n",
       " '연필사건 학생의 학부모가 고인의 휴대전화 번호를 알게 된 경위나 담임 자격 시비와 같은 폭언이 있었는지 여부, 학교 쪽이 연필사건을 원만히 중재했다고 한 7월13일 이후에도 추가적인 학부모 민원이 있었는지 등 정작 규명이 필요한 의혹에 대해서는 새롭게 밝혀낸 것이 하나도 없다.',\n",
       " '무엇보다 합동조사 결과에는 고인이 사망에 이르기까지 학교나 학교장의 책임은 없었는지에 대한 내용이 쏙 빠져 있다.',\n",
       " '고인은 연필사건 학생뿐 아니라 다른 2명의 부적응 학생으로 인한 고충이 적지 않았고, 모두 10차례나 학교 쪽에 학생 지도의 어려움을 이유로 상담 요청을 한 바 있다.',\n",
       " '그러나 학교 쪽은 고인에게 얼른 전화번호를 바꾸라거나 학부모에게 심리검사 또는 상담을 받을 것을 권유하라고 조언하는 정도에 그쳤다.',\n",
       " '학교가 함께 문제를 해결하려는 의지를 보이는 대신 개별 교사에게 책임을 지운 정황이 아닐 수 없다.',\n",
       " '심지어 학교 쪽은 지난달 최초 작성한 입장문에서 연필사건이 원만히 해결된 것처럼 언급했다가 해당 내용을 삭제했는데, 애초 어떤 의도로 작성한 것인지 규명될 필요가 있다.',\n",
       " '교육당국은 재발 방지 대책을 촘촘하게 마련하는 일도 중요하지만 제대로 된 진상규명이 우선이라는 점을 명심해야 한다.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kss\n",
    "text = kss.split_sentences(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def check_token(text):\n",
    "        #data = list(filter(None, text.split('\\n')))\n",
    "        #data = split_sentences(text)\n",
    "        bertdata = BertData()\n",
    "        txt_data = bertdata.preprocess(text)\n",
    "        Embedding_scr = txt_data[0]\n",
    "        print('입력 받은 문장 길이:',len(text))\n",
    "        print(Embedding_scr)\n",
    "        print('총 임베딩 길이:',len(Embedding_scr))\n",
    "        return Embedding_scr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 받은 문장 길이: 39\n",
      "[2, 3417, 3686, 3417, 3941, 2631, 3941, 2968, 4223, 2629, 2629, 3388, 3388, 3153, 3621, 2889, 4224, 2629, 3714, 2885, 2632, 4228, 2930, 3789, 3388, 2596, 2618, 2542, 3833, 2763, 524, 3732, 2760, 3667, 3833, 3145, 3400, 3099, 2585, 2653, 3190, 2936, 2579, 3729, 3614, 2873, 2856, 2629, 3388, 2963, 3728, 3245, 3247, 3729, 2572, 3429, 2873, 518, 3, 2, 2629, 3714, 3308, 3661, 3417, 3686, 3506, 2629, 3714, 3931, 3729, 3388, 2574, 3834, 4293, 3388, 3510, 2620, 2603, 4280, 3731, 3621, 2760, 3417, 2590, 2873, 3185, 4228, 2930, 3789, 3388, 3621, 3903, 3468, 4239, 3833, 3145, 516, 2889, 3308, 3310, 3728, 4271, 3723, 2601, 3906, 3468, 3388, 3094, 3230, 3115, 2942, 3187, 3417, 700, 3681, 2942, 3388, 3230, 3789, 3388, 701, 2542, 2935, 2605, 3148, 3580, 2873, 2856, 2579, 3729, 2873, 518, 3, 2, 3768, 2632, 3941, 2968, 2629, 3388, 2821, 3789, 2856, 702, 700, 3400, 2776, 2666, 2629, 3388, 3728, 3809, 3725, 3621, 3203, 2572, 3685, 3914, 3735, 2548, 3723, 2854, 2755, 2873, 701, 3185, 2776, 2830, 3722, 2596, 2618, 3042, 2605, 2768, 2960, 4225, 3468, 3614, 3723, 3774, 2922, 3094, 4243, 3472, 4222, 2873, 518, 4228, 2930, 3789, 3388, 2876, 3729, 4232, 3593, 4225, 3732, 3722, 4232, 2885, 4223, 2629, 2542, 2778, 2542, 3774, 4111, 3508, 3206, 2776, 3681, 3728, 3388, 3510, 4280, 3731, 3729, 3569, 2866, 2605, 3731, 3728, 3613, 3203, 3398, 2605, 3961, 3723, 3187, 3187, 4327, 2615, 2559, 4222, 2856, 2579, 3729, 3605, 3593, 4239, 2873, 703, 3185, 3754, 3789, 3388, 3128, 3942, 2631, 4239, 2873, 518, 3, 2, 3768, 2632, 2629, 3388, 524, 3145, 3190, 3722, 3833, 2763, 3808, 3148, 3621, 2922, 3927, 3766, 4224, 3835, 3398, 2653, 3190, 3723, 3942, 2631, 4222, 2856, 3839, 4288, 3128, 3729, 3605, 2552, 2873, 518, 3, 2, 3510, 3776, 3094, 4228, 2930, 3789, 3388, 2596, 2618, 3728, 2889, 3308, 3310, 3722, 4223, 2629, 3873, 3729, 2605, 3731, 3728, 3388, 3153, 3834, 4293, 2783, 2901, 3736, 3752, 3206, 2618, 3607, 3096, 3290, 2922, 3621, 3417, 3776, 2666, 2936, 2776, 3681, 3729, 3154, 2856, 3833, 4280, 3731, 4222, 2856, 3468, 3810, 3621, 2656, 3940, 2873, 518, 3, 2, 3652, 4232, 523, 3694, 3729, 4293, 2605, 3731, 3728, 4223, 2662, 2882, 3735, 2629, 3932, 3388, 3510, 3729, 3614, 3616, 2873, 2572, 2760, 516, 4232, 2885, 4223, 2662, 3621, 3417, 3652, 4232, 4223, 2629, 4189, 3084, 3508, 2605, 2542, 3614, 3616, 3720, 3185, 516, 4223, 2662, 2776, 3729, 3127, 3242, 700, 3632, 4217, 3388, 2574, 701, 3729, 4293, 2605, 3731, 3729, 4223, 3308, 3192, 3094, 3308, 4084, 3768, 4279, 3128, 3246, 3580, 2873, 2856, 2968, 3728, 2876, 3470, 3388, 3510, 4280, 3731, 3621, 3311, 2618, 4222, 2873, 518, 3, 2, 3632, 4217, 3388, 2574, 4223, 3407, 3728, 4223, 3308, 3192, 2542, 2605, 3731, 3728, 4312, 2889, 3768, 4279, 3266, 4270, 3128, 3574, 2585, 2936, 2601, 3705, 2760, 2882, 3735, 3743, 2593, 3506, 3324, 3661, 2556, 3722, 4189, 3607, 3729, 3738, 3616, 2856, 3833, 3629, 3308, 516, 4223, 2629, 3873, 3729, 3632, 4217, 3388, 2574, 3723, 3693, 3145, 4327, 3815, 3754, 4239, 2873, 2605, 4224, 527, 3694, 521, 523, 3732, 3729, 4293, 3621, 2922, 3954, 2542, 3767, 3731, 4223, 3308, 3192, 3232, 3693, 3729, 3738, 3616, 2856, 3833, 2968, 3774, 3744, 2653, 3190, 3729, 4217, 3674, 4224, 3728, 4271, 3621, 2889, 4232, 3417, 2856, 3400, 3099, 2585, 3248, 4260, 2778, 2579, 3729, 4222, 2760, 2922, 3614, 2873, 518, 3, 2, 3203, 3615, 3290, 2873, 4228, 2930, 3789, 3388, 2596, 2618, 3621, 2856, 2605, 3731, 3729, 3388, 3153, 3621, 3729, 3125, 2666, 2677, 3833, 4223, 2629, 2760, 4223, 2629, 3752, 3728, 3914, 3735, 3722, 3614, 3616, 2856, 3833, 3621, 2889, 4224, 2776, 3681, 3729, 3541, 3334, 3783, 3738, 2873, 518, 3, 2, 2605, 3731, 3722, 3632, 4217, 3388, 2574, 4223, 3407, 3375, 3569, 2864, 3042, 2873, 3127, 522, 3190, 3728, 3308, 3767, 3727, 4223, 3407, 3720, 3094, 3731, 4224, 2605, 3961, 3729, 3767, 3833, 3573, 3580, 2605, 516, 3192, 2942, 521, 520, 3902, 3092, 2760, 4223, 2629, 3873, 3621, 4223, 3407, 3833, 2922, 3728, 3605, 3083, 3687, 3723, 3729, 3713, 3094, 3398, 2882, 3674, 3931, 3723, 4224, 3242, 3738, 2873, 518, 3, 2, 2656, 3065, 2760, 4223, 2629, 3873, 3722, 2605, 3731, 3621, 2585, 3610, 3127, 3768, 4279, 3266, 4270, 3128, 3242, 2725, 3042, 2572, 2760, 4223, 3308, 3192, 3621, 2585, 3512, 3135, 2577, 3388, 3010, 2856, 3398, 2882, 3723, 3246, 3723, 2579, 3723, 2644, 3713, 4222, 3042, 2605, 3789, 3607, 4222, 2856, 3774, 2922, 3621, 2656, 3940, 2873, 518, 3, 2, 4223, 2629, 2542, 4227, 2705, 3206, 3776, 3128, 4232, 2596, 4222, 3083, 2856, 3728, 3833, 3128, 3290, 3729, 2856, 2889, 3508, 2559, 3283, 2629, 3388, 3621, 2585, 3914, 3735, 3723, 3833, 3685, 3774, 4284, 3729, 3569, 2867, 3468, 3614, 2873, 518, 3, 2, 3512, 3833, 3605, 4223, 2629, 3873, 3722, 3833, 2763, 2878, 3952, 3941, 3744, 3427, 4224, 3736, 3752, 3206, 3621, 3417, 3632, 4217, 3388, 2574, 3729, 3693, 3145, 4327, 4232, 2596, 2936, 2579, 3924, 3069, 3607, 2662, 4239, 2873, 2542, 4232, 2885, 2776, 3681, 3723, 3389, 3776, 4239, 2856, 2910, 516, 3584, 3941, 3605, 2998, 3728, 2922, 3094, 3744, 3427, 4224, 2579, 3731, 3833, 2653, 3190, 2937, 4217, 3674, 2542, 3738, 2873, 518, 3, 2, 2629, 3714, 2885, 2632, 3722, 3754, 3247, 3253, 3833, 2889, 3914, 3723, 3945, 3945, 4222, 2585, 3143, 3085, 4222, 2856, 3732, 2922, 3815, 3674, 4222, 3833, 3145, 3776, 2889, 3094, 2936, 3835, 3398, 2653, 3190, 3729, 3683, 3420, 3729, 3042, 2856, 3771, 3723, 3190, 3512, 4232, 3593, 4224, 2873, 518, 3, 2, 3417, 3686, 3417, 3941, 2631, 3941, 2968, 4223, 2629, 2629, 3388, 3388, 3153, 3621, 2889, 4224, 2629, 3714, 2885, 2632, 4228, 2930, 3789, 3388, 2596, 2618, 2542, 3833, 2763, 524, 3732, 2760, 3667, 3833, 3145, 3400, 3099, 2585, 2653, 3190, 2936, 2579, 3729, 3614, 2873, 2856, 2629, 3388, 2963, 3728, 3245, 3247, 3729, 2572, 3429, 2873, 518, 3, 2, 2629, 3714, 3308, 3661, 3417, 3686, 3506, 2629, 3714, 3931, 3729, 3388, 2574, 3834, 4293, 3388, 3510, 2620, 2603, 4280, 3731, 3621, 2760, 3417, 2590, 2873, 3185, 4228, 2930, 3789, 3388, 3621, 3903, 3468, 4239, 3833, 3145, 516, 2889, 3308, 3310, 3728, 4271, 3723, 2601, 3906, 3468, 3388, 3094, 3230, 3115, 2942, 3187, 3417, 700, 3681, 2942, 3388, 3230, 3789, 3388, 701, 2542, 2935, 2605, 3148, 3580, 2873, 2856, 2579, 3729, 2873, 518, 3, 2, 3768, 2632, 3941, 2968, 2629, 3388, 2821, 3789, 2856, 702, 700, 3400, 2776, 2666, 2629, 3388, 3728, 3809, 3725, 3621, 3203, 2572, 3685, 3914, 3735, 2548, 3723, 2854, 2755, 2873, 701, 3185, 2776, 2830, 3722, 2596, 2618, 3042, 2605, 2768, 2960, 4225, 3468, 3614, 3723, 3774, 2922, 3094, 4243, 3472, 4222, 2873, 518, 4228, 2930, 3789, 3388, 2876, 3729, 4232, 3593, 4225, 3732, 3722, 4232, 2885, 4223, 2629, 2542, 2778, 2542, 3774, 4111, 3508, 3206, 2776, 3681, 3728, 3388, 3510, 4280, 3731, 3729, 3569, 2866, 2605, 3731, 3728, 3613, 3203, 3398, 2605, 3961, 3723, 3187, 3187, 4327, 2615, 2559, 4222, 2856, 2579, 3729, 3605, 3593, 4239, 2873, 703, 3185, 3754, 3789, 3388, 3128, 3942, 2631, 4239, 2873, 518, 3, 2, 3768, 2632, 2629, 3388, 524, 3145, 3190, 3722, 3833, 2763, 3808, 3148, 3621, 2922, 3927, 3766, 4224, 3835, 3398, 2653, 3190, 3723, 3942, 2631, 4222, 2856, 3839, 4288, 3128, 3729, 3605, 2552, 2873, 518, 3, 2, 3510, 3776, 3094, 4228, 2930, 3789, 3388, 2596, 2618, 3728, 2889, 3308, 3310, 3722, 4223, 2629, 3873, 3729, 2605, 3731, 3728, 3388, 3153, 3834, 4293, 2783, 2901, 3736, 3752, 3206, 2618, 3607, 3096, 3290, 2922, 3621, 3417, 3776, 2666, 2936, 2776, 3681, 3729, 3154, 2856, 3833, 4280, 3731, 4222, 2856, 3468, 3810, 3621, 2656, 3940, 2873, 518, 3, 2, 3652, 4232, 523, 3694, 3729, 4293, 2605, 3731, 3728, 4223, 2662, 2882, 3735, 2629, 3932, 3388, 3510, 3729, 3614, 3616, 2873, 2572, 2760, 516, 4232, 2885, 4223, 2662, 3621, 3417, 3652, 4232, 4223, 2629, 4189, 3084, 3508, 2605, 2542, 3614, 3616, 3720, 3185, 516, 4223, 2662, 2776, 3729, 3127, 3242, 700, 3632, 4217, 3388, 2574, 701, 3729, 4293, 2605, 3731, 3729, 4223, 3308, 3192, 3094, 3308, 4084, 3768, 4279, 3128, 3246, 3580, 2873, 2856, 2968, 3728, 2876, 3470, 3388, 3510, 4280, 3731, 3621, 3311, 2618, 4222, 2873, 518, 3, 2, 3632, 4217, 3388, 2574, 4223, 3407, 3728, 4223, 3308, 3192, 2542, 2605, 3731, 3728, 4312, 2889, 3768, 4279, 3266, 4270, 3128, 3574, 2585, 2936, 2601, 3705, 2760, 2882, 3735, 3743, 2593, 3506, 3324, 3661, 2556, 3722, 4189, 3607, 3729, 3738, 3616, 2856, 3833, 3629, 3308, 516, 4223, 2629, 3873, 3729, 3632, 4217, 3388, 2574, 3723, 3693, 3145, 4327, 3815, 3754, 4239, 2873, 2605, 4224, 527, 3694, 521, 523, 3732, 3729, 4293, 3621, 2922, 3954, 2542, 3767, 3731, 4223, 3308, 3192, 3232, 3693, 3729, 3738, 3616, 2856, 3833, 2968, 3774, 3744, 2653, 3190, 3729, 4217, 3674, 4224, 3728, 4271, 3621, 2889, 4232, 3417, 2856, 3400, 3099, 2585, 3248, 4260, 2778, 2579, 3729, 4222, 2760, 2922, 3614, 2873, 518, 3, 2, 3203, 3615, 3290, 2873, 4228, 2930, 3789, 3388, 2596, 2618, 3621, 2856, 2605, 3731, 3729, 3388, 3153, 3621, 3729, 3125, 2666, 2677, 3833, 4223, 2629, 2760, 4223, 2629, 3752, 3728, 3914, 3735, 3722, 3614, 3616, 2856, 3833, 3621, 2889, 4224, 2776, 3681, 3729, 3541, 3334, 3783, 3738, 2873, 518, 3, 2, 2605, 3731, 3722, 3632, 4217, 3388, 2574, 4223, 3407, 3375, 3569, 2864, 3042, 2873, 3127, 522, 3190, 3728, 3308, 3767, 3727, 4223, 3407, 3720, 3094, 3731, 4224, 2605, 3961, 3729, 3767, 3833, 3573, 3580, 2605, 516, 3192, 2942, 521, 520, 3902, 3092, 2760, 4223, 2629, 3873, 3621, 4223, 3407, 3833, 2922, 3728, 3605, 3083, 3687, 3723, 3729, 3713, 3094, 3398, 2882, 3674, 3931, 3723, 4224, 3242, 3738, 2873, 518, 3, 2, 2656, 3065, 2760, 4223, 2629, 3873, 3722, 2605, 3731, 3621, 2585, 3610, 3127, 3768, 4279, 3266, 4270, 3128, 3242, 2725, 3042, 2572, 2760, 4223, 3308, 3192, 3621, 2585, 3512, 3135, 2577, 3388, 3010, 2856, 3398, 2882, 3723, 3246, 3723, 2579, 3723, 2644, 3713, 4222, 3042, 2605, 3789, 3607, 4222, 2856, 3774, 2922, 3621, 2656, 3940, 2873, 518, 3, 2, 4223, 2629, 2542, 4227, 2705, 3206, 3776, 3128, 4232, 2596, 4222, 3083, 2856, 3728, 3833, 3128, 3290, 3729, 2856, 2889, 3508, 2559, 3283, 2629, 3388, 3621, 2585, 3914, 3735, 3723, 3833, 3685, 3774, 4284, 3729, 3569, 2867, 3468, 3614, 2873, 518, 3, 2, 3512, 3833, 3605, 4223, 2629, 3873, 3722, 3833, 2763, 2878, 3952, 3941, 3744, 3427, 4224, 3736, 3752, 3206, 3621, 3417, 3632, 4217, 3388, 2574, 3729, 3693, 3145, 4327, 4232, 2596, 2936, 2579, 3924, 3069, 3607, 2662, 4239, 2873, 2542, 4232, 2885, 2776, 3681, 3723, 3389, 3776, 4239, 2856, 2910, 516, 3584, 3941, 3605, 2998, 3728, 2922, 3094, 3744, 3427, 4224, 2579, 3731, 3833, 2653, 3190, 2937, 4217, 3674, 2542, 3738, 2873, 518, 3, 2, 2629, 3714, 2885, 2632, 3722, 3754, 3247, 3253, 3833, 2889, 3914, 3723, 3945, 3945, 4222, 2585, 3143, 3085, 4222, 2856, 3732, 2922, 3815, 3674, 4222, 3833, 3145, 3776, 2889, 3094, 2936, 3835, 3398, 2653, 3190, 3729, 3683, 3420, 3729, 3042, 2856, 3771, 3723, 3190, 3512, 4232, 3593, 4224, 2873, 518, 3, 2, 3417, 3686, 3417, 3941, 2631, 3941, 2968, 4223, 2629, 2629, 3388, 3388, 3153, 3621, 2889, 4224, 2629, 3714, 2885, 2632, 4228, 2930, 3789, 3388, 2596, 2618, 2542, 3833, 2763, 524, 3732, 2760, 3667, 3833, 3145, 3400, 3099, 2585, 2653, 3190, 2936, 2579, 3729, 3614, 2873, 2856, 2629, 3388, 2963, 3728, 3245, 3247, 3729, 2572, 3429, 2873, 518, 3, 2, 2629, 3714, 3308, 3661, 3417, 3686, 3506, 2629, 3714, 3931, 3729, 3388, 2574, 3834, 4293, 3388, 3510, 2620, 2603, 4280, 3731, 3621, 2760, 3417, 2590, 2873, 3185, 4228, 2930, 3789, 3388, 3621, 3903, 3468, 4239, 3833, 3145, 516, 2889, 3308, 3310, 3728, 4271, 3723, 2601, 3906, 3468, 3388, 3094, 3230, 3115, 2942, 3187, 3417, 700, 3681, 2942, 3388, 3230, 3789, 3388, 701, 2542, 2935, 2605, 3148, 3580, 2873, 2856, 2579, 3729, 2873, 518, 3, 2, 3768, 2632, 3941, 2968, 2629, 3388, 2821, 3789, 2856, 702, 700, 3400, 2776, 2666, 2629, 3388, 3728, 3809, 3725, 3621, 3203, 2572, 3685, 3914, 3735, 2548, 3723, 2854, 2755, 2873, 701, 3185, 2776, 2830, 3722, 2596, 2618, 3042, 2605, 2768, 2960, 4225, 3468, 3614, 3723, 3774, 2922, 3094, 4243, 3472, 4222, 2873, 518, 4228, 2930, 3789, 3388, 2876, 3729, 4232, 3593, 4225, 3732, 3722, 4232, 2885, 4223, 2629, 2542, 2778, 2542, 3774, 4111, 3508, 3206, 2776, 3681, 3728, 3388, 3510, 4280, 3731, 3729, 3569, 2866, 2605, 3731, 3728, 3613, 3203, 3398, 2605, 3961, 3723, 3187, 3187, 4327, 2615, 2559, 4222, 2856, 2579, 3729, 3605, 3593, 4239, 2873, 703, 3185, 3754, 3789, 3388, 3128, 3942, 2631, 4239, 2873, 518, 3, 2, 3768, 2632, 2629, 3388, 524, 3145, 3190, 3722, 3833, 2763, 3808, 3148, 3621, 2922, 3927, 3766, 4224, 3835, 3398, 2653, 3190, 3723, 3942, 2631, 4222, 2856, 3839, 4288, 3128, 3729, 3605, 2552, 2873, 518, 3, 2, 3510, 3776, 3094, 4228, 2930, 3789, 3388, 2596, 2618, 3728, 2889, 3308, 3310, 3722, 4223, 2629, 3873, 3729, 2605, 3731, 3728, 3388, 3153, 3834, 4293, 2783, 2901, 3736, 3752, 3206, 2618, 3607, 3096, 3290, 2922, 3621, 3417, 3776, 2666, 2936, 2776, 3681, 3729, 3154, 2856, 3833, 4280, 3731, 4222, 2856, 3468, 3810, 3621, 2656, 3940, 2873, 518, 3, 2, 3652, 4232, 523, 3694, 3729, 4293, 2605, 3731, 3728, 4223, 2662, 2882, 3735, 2629, 3932, 3388, 3510, 3729, 3614, 3616, 2873, 2572, 2760, 516, 4232, 2885, 4223, 2662, 3621, 3417, 3652, 4232, 4223, 2629, 4189, 3084, 3508, 2605, 2542, 3614, 3616, 3720, 3185, 516, 4223, 2662, 2776, 3729, 3127, 3242, 700, 3632, 4217, 3388, 2574, 701, 3729, 4293, 2605, 3731, 3729, 4223, 3308, 3192, 3094, 3308, 4084, 3768, 4279, 3128, 3246, 3580, 2873, 2856, 2968, 3728, 2876, 3470, 3388, 3510, 4280, 3731, 3621, 3311, 2618, 4222, 2873, 518, 3, 2, 3632, 4217, 3388, 2574, 4223, 3407, 3728, 4223, 3308, 3192, 2542, 2605, 3731, 3728, 4312, 2889, 3768, 4279, 3266, 4270, 3128, 3574, 2585, 2936, 2601, 3705, 2760, 2882, 3735, 3743, 2593, 3506, 3324, 3661, 2556, 3722, 4189, 3607, 3729, 3738, 3616, 2856, 3833, 3629, 3308, 516, 4223, 2629, 3873, 3729, 3632, 4217, 3388, 2574, 3723, 3693, 3145, 4327, 3815, 3754, 4239, 2873, 2605, 4224, 527, 3694, 521, 523, 3732, 3729, 4293, 3621, 2922, 3954, 2542, 3767, 3731, 4223, 3308, 3192, 3232, 3693, 3729, 3738, 3616, 2856, 3833, 2968, 3774, 3744, 2653, 3190, 3729, 4217, 3674, 4224, 3728, 4271, 3621, 2889, 4232, 3417, 2856, 3400, 3099, 2585, 3248, 4260, 2778, 2579, 3729, 4222, 2760, 2922, 3614, 2873, 518, 3, 2, 3203, 3615, 3290, 2873, 4228, 2930, 3789, 3388, 2596, 2618, 3621, 2856, 2605, 3731, 3729, 3388, 3153, 3621, 3729, 3125, 2666, 2677, 3833, 4223, 2629, 2760, 4223, 2629, 3752, 3728, 3914, 3735, 3722, 3614, 3616, 2856, 3833, 3621, 2889, 4224, 2776, 3681, 3729, 3541, 3334, 3783, 3738, 2873, 518, 3, 2, 2605, 3731, 3722, 3632, 4217, 3388, 2574, 4223, 3407, 3375, 3569, 2864, 3042, 2873, 3127, 522, 3190, 3728, 3308, 3767, 3727, 4223, 3407, 3720, 3094, 3731, 4224, 2605, 3961, 3729, 3767, 3833, 3573, 3580, 2605, 516, 3192, 2942, 521, 520, 3902, 3092, 2760, 4223, 2629, 3873, 3621, 4223, 3407, 3833, 2922, 3728, 3605, 3083, 3687, 3723, 3729, 3713, 3094, 3398, 2882, 3674, 3931, 3723, 4224, 3242, 3738, 2873, 518, 3, 2, 2656, 3065, 2760, 4223, 2629, 3873, 3722, 2605, 3731, 3621, 2585, 3610, 3127, 3768, 4279, 3266, 4270, 3128, 3242, 2725, 3042, 2572, 2760, 4223, 3308, 3192, 3621, 2585, 3512, 3135, 2577, 3388, 3010, 2856, 3398, 2882, 3723, 3246, 3723, 2579, 3723, 2644, 3713, 4222, 3042, 2605, 3789, 3607, 4222, 2856, 3774, 2922, 3621, 2656, 3940, 2873, 518, 3, 2, 4223, 2629, 2542, 4227, 2705, 3206, 3776, 3128, 4232, 2596, 4222, 3083, 2856, 3728, 3833, 3128, 3290, 3729, 2856, 2889, 3508, 2559, 3283, 2629, 3388, 3621, 2585, 3914, 3735, 3723, 3833, 3685, 3774, 4284, 3729, 3569, 2867, 3468, 3614, 2873, 518, 3, 2, 3512, 3833, 3605, 4223, 2629, 3873, 3722, 3833, 2763, 2878, 3952, 3941, 3744, 3427, 4224, 3736, 3752, 3206, 3621, 3417, 3632, 4217, 3388, 2574, 3729, 3693, 3145, 4327, 4232, 2596, 2936, 2579, 3924, 3069, 3607, 2662, 4239, 2873, 2542, 4232, 2885, 2776, 3681, 3723, 3389, 3776, 4239, 2856, 2910, 516, 3584, 3941, 3605, 2998, 3728, 2922, 3094, 3744, 3427, 4224, 2579, 3731, 3833, 2653, 3190, 2937, 4217, 3674, 2542, 3738, 2873, 518, 3, 2, 2629, 3714, 2885, 2632, 3722, 3754, 3247, 3253, 3833, 2889, 3914, 3723, 3945, 3945, 4222, 2585, 3143, 3085, 4222, 2856, 3732, 2922, 3815, 3674, 4222, 3833, 3145, 3776, 2889, 3094, 2936, 3835, 3398, 2653, 3190, 3729, 3683, 3420, 3729, 3042, 2856, 3771, 3723, 3190, 3512, 4232, 3593, 4224, 2873, 518, 3]\n",
      "총 임베딩 길이: 2724\n"
     ]
    }
   ],
   "source": [
    "check_token_size = check_token(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# data_dict = {\"src\":txt_data[0],  'src': [2,3417,..., 3, 2,...., 3]\n",
    "#             \"labels\":[0,1,2],     labels': [0, 1, 2],\n",
    "#             \"segs\":txt_data[2],  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ....]\n",
    "#             \"clss\":txt_data[3],     'clss': [0, 59, 134, 254, 290, 349, 439, 558, 609, 680, 739, 782, 856],      \n",
    "#             \"src_txt\":txt_data[4],        '서 울   서 초 구   초 등 학 교   교 사   사 망 에 \n",
    "#             \"tgt_txt\":None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = txt2input(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': [2,\n",
       "  3417,\n",
       "  3686,\n",
       "  3417,\n",
       "  3941,\n",
       "  2631,\n",
       "  3941,\n",
       "  2968,\n",
       "  4223,\n",
       "  2629,\n",
       "  2629,\n",
       "  3388,\n",
       "  3388,\n",
       "  3153,\n",
       "  3621,\n",
       "  2889,\n",
       "  4224,\n",
       "  2629,\n",
       "  3714,\n",
       "  2885,\n",
       "  2632,\n",
       "  4228,\n",
       "  2930,\n",
       "  3789,\n",
       "  3388,\n",
       "  2596,\n",
       "  2618,\n",
       "  2542,\n",
       "  3833,\n",
       "  2763,\n",
       "  524,\n",
       "  3732,\n",
       "  2760,\n",
       "  3667,\n",
       "  3833,\n",
       "  3145,\n",
       "  3400,\n",
       "  3099,\n",
       "  2585,\n",
       "  2653,\n",
       "  3190,\n",
       "  2936,\n",
       "  2579,\n",
       "  3729,\n",
       "  3614,\n",
       "  2873,\n",
       "  2856,\n",
       "  2629,\n",
       "  3388,\n",
       "  2963,\n",
       "  3728,\n",
       "  3245,\n",
       "  3247,\n",
       "  3729,\n",
       "  2572,\n",
       "  3429,\n",
       "  2873,\n",
       "  518,\n",
       "  3,\n",
       "  2,\n",
       "  2629,\n",
       "  3714,\n",
       "  3308,\n",
       "  3661,\n",
       "  3417,\n",
       "  3686,\n",
       "  3506,\n",
       "  2629,\n",
       "  3714,\n",
       "  3931,\n",
       "  3729,\n",
       "  3388,\n",
       "  2574,\n",
       "  3834,\n",
       "  4293,\n",
       "  3388,\n",
       "  3510,\n",
       "  2620,\n",
       "  2603,\n",
       "  4280,\n",
       "  3731,\n",
       "  3621,\n",
       "  2760,\n",
       "  3417,\n",
       "  2590,\n",
       "  2873,\n",
       "  3185,\n",
       "  4228,\n",
       "  2930,\n",
       "  3789,\n",
       "  3388,\n",
       "  3621,\n",
       "  3903,\n",
       "  3468,\n",
       "  4239,\n",
       "  3833,\n",
       "  3145,\n",
       "  516,\n",
       "  2889,\n",
       "  3308,\n",
       "  3310,\n",
       "  3728,\n",
       "  4271,\n",
       "  3723,\n",
       "  2601,\n",
       "  3906,\n",
       "  3468,\n",
       "  3388,\n",
       "  3094,\n",
       "  3230,\n",
       "  3115,\n",
       "  2942,\n",
       "  3187,\n",
       "  3417,\n",
       "  700,\n",
       "  3681,\n",
       "  2942,\n",
       "  3388,\n",
       "  3230,\n",
       "  3789,\n",
       "  3388,\n",
       "  701,\n",
       "  2542,\n",
       "  2935,\n",
       "  2605,\n",
       "  3148,\n",
       "  3580,\n",
       "  2873,\n",
       "  2856,\n",
       "  2579,\n",
       "  3729,\n",
       "  2873,\n",
       "  518,\n",
       "  3,\n",
       "  2,\n",
       "  3768,\n",
       "  2632,\n",
       "  3941,\n",
       "  2968,\n",
       "  2629,\n",
       "  3388,\n",
       "  2821,\n",
       "  3789,\n",
       "  2856,\n",
       "  702,\n",
       "  700,\n",
       "  3400,\n",
       "  2776,\n",
       "  2666,\n",
       "  2629,\n",
       "  3388,\n",
       "  3728,\n",
       "  3809,\n",
       "  3725,\n",
       "  3621,\n",
       "  3203,\n",
       "  2572,\n",
       "  3685,\n",
       "  3914,\n",
       "  3735,\n",
       "  2548,\n",
       "  3723,\n",
       "  2854,\n",
       "  2755,\n",
       "  2873,\n",
       "  701,\n",
       "  3185,\n",
       "  2776,\n",
       "  2830,\n",
       "  3722,\n",
       "  2596,\n",
       "  2618,\n",
       "  3042,\n",
       "  2605,\n",
       "  2768,\n",
       "  2960,\n",
       "  4225,\n",
       "  3468,\n",
       "  3614,\n",
       "  3723,\n",
       "  3774,\n",
       "  2922,\n",
       "  3094,\n",
       "  4243,\n",
       "  3472,\n",
       "  4222,\n",
       "  2873,\n",
       "  518,\n",
       "  4228,\n",
       "  2930,\n",
       "  3789,\n",
       "  3388,\n",
       "  2876,\n",
       "  3729,\n",
       "  4232,\n",
       "  3593,\n",
       "  4225,\n",
       "  3732,\n",
       "  3722,\n",
       "  4232,\n",
       "  2885,\n",
       "  4223,\n",
       "  2629,\n",
       "  2542,\n",
       "  2778,\n",
       "  2542,\n",
       "  3774,\n",
       "  4111,\n",
       "  3508,\n",
       "  3206,\n",
       "  2776,\n",
       "  3681,\n",
       "  3728,\n",
       "  3388,\n",
       "  3510,\n",
       "  4280,\n",
       "  3731,\n",
       "  3729,\n",
       "  3569,\n",
       "  2866,\n",
       "  2605,\n",
       "  3731,\n",
       "  3728,\n",
       "  3613,\n",
       "  3203,\n",
       "  3398,\n",
       "  2605,\n",
       "  3961,\n",
       "  3723,\n",
       "  3187,\n",
       "  3187,\n",
       "  4327,\n",
       "  2615,\n",
       "  2559,\n",
       "  4222,\n",
       "  2856,\n",
       "  2579,\n",
       "  3729,\n",
       "  3605,\n",
       "  3593,\n",
       "  4239,\n",
       "  2873,\n",
       "  703,\n",
       "  3185,\n",
       "  3754,\n",
       "  3789,\n",
       "  3388,\n",
       "  3128,\n",
       "  3942,\n",
       "  2631,\n",
       "  4239,\n",
       "  2873,\n",
       "  518,\n",
       "  3,\n",
       "  2,\n",
       "  3768,\n",
       "  2632,\n",
       "  2629,\n",
       "  3388,\n",
       "  524,\n",
       "  3145,\n",
       "  3190,\n",
       "  3722,\n",
       "  3833,\n",
       "  2763,\n",
       "  3808,\n",
       "  3148,\n",
       "  3621,\n",
       "  2922,\n",
       "  3927,\n",
       "  3766,\n",
       "  4224,\n",
       "  3835,\n",
       "  3398,\n",
       "  2653,\n",
       "  3190,\n",
       "  3723,\n",
       "  3942,\n",
       "  2631,\n",
       "  4222,\n",
       "  2856,\n",
       "  3839,\n",
       "  4288,\n",
       "  3128,\n",
       "  3729,\n",
       "  3605,\n",
       "  2552,\n",
       "  2873,\n",
       "  518,\n",
       "  3,\n",
       "  2,\n",
       "  3510,\n",
       "  3776,\n",
       "  3094,\n",
       "  4228,\n",
       "  2930,\n",
       "  3789,\n",
       "  3388,\n",
       "  2596,\n",
       "  2618,\n",
       "  3728,\n",
       "  2889,\n",
       "  3308,\n",
       "  3310,\n",
       "  3722,\n",
       "  4223,\n",
       "  2629,\n",
       "  3873,\n",
       "  3729,\n",
       "  2605,\n",
       "  3731,\n",
       "  3728,\n",
       "  3388,\n",
       "  3153,\n",
       "  3834,\n",
       "  4293,\n",
       "  2783,\n",
       "  2901,\n",
       "  3736,\n",
       "  3752,\n",
       "  3206,\n",
       "  2618,\n",
       "  3607,\n",
       "  3096,\n",
       "  3290,\n",
       "  2922,\n",
       "  3621,\n",
       "  3417,\n",
       "  3776,\n",
       "  2666,\n",
       "  2936,\n",
       "  2776,\n",
       "  3681,\n",
       "  3729,\n",
       "  3154,\n",
       "  2856,\n",
       "  3833,\n",
       "  4280,\n",
       "  3731,\n",
       "  4222,\n",
       "  2856,\n",
       "  3468,\n",
       "  3810,\n",
       "  3621,\n",
       "  2656,\n",
       "  3940,\n",
       "  2873,\n",
       "  518,\n",
       "  3,\n",
       "  2,\n",
       "  3652,\n",
       "  4232,\n",
       "  523,\n",
       "  3694,\n",
       "  3729,\n",
       "  4293,\n",
       "  2605,\n",
       "  3731,\n",
       "  3728,\n",
       "  4223,\n",
       "  2662,\n",
       "  2882,\n",
       "  3735,\n",
       "  2629,\n",
       "  3932,\n",
       "  3388,\n",
       "  3510,\n",
       "  3729,\n",
       "  3614,\n",
       "  3616,\n",
       "  2873,\n",
       "  2572,\n",
       "  2760,\n",
       "  516,\n",
       "  4232,\n",
       "  2885,\n",
       "  4223,\n",
       "  2662,\n",
       "  3621,\n",
       "  3417,\n",
       "  3652,\n",
       "  4232,\n",
       "  4223,\n",
       "  2629,\n",
       "  4189,\n",
       "  3084,\n",
       "  3508,\n",
       "  2605,\n",
       "  2542,\n",
       "  3614,\n",
       "  3616,\n",
       "  3720,\n",
       "  3185,\n",
       "  516,\n",
       "  4223,\n",
       "  2662,\n",
       "  2776,\n",
       "  3729,\n",
       "  3127,\n",
       "  3242,\n",
       "  700,\n",
       "  3632,\n",
       "  4217,\n",
       "  3388,\n",
       "  2574,\n",
       "  701,\n",
       "  3729,\n",
       "  4293,\n",
       "  2605,\n",
       "  3731,\n",
       "  3729,\n",
       "  4223,\n",
       "  3308,\n",
       "  3192,\n",
       "  3094,\n",
       "  3308,\n",
       "  4084,\n",
       "  3768,\n",
       "  4279,\n",
       "  3128,\n",
       "  3246,\n",
       "  3580,\n",
       "  2873,\n",
       "  2856,\n",
       "  2968,\n",
       "  3728,\n",
       "  2876,\n",
       "  3470,\n",
       "  3388,\n",
       "  3510,\n",
       "  4280,\n",
       "  3731,\n",
       "  3621,\n",
       "  3311,\n",
       "  2618,\n",
       "  4222,\n",
       "  2873,\n",
       "  518,\n",
       "  3,\n",
       "  2,\n",
       "  3632,\n",
       "  4217,\n",
       "  3388,\n",
       "  2574,\n",
       "  4223,\n",
       "  3407,\n",
       "  3728,\n",
       "  4223,\n",
       "  3308,\n",
       "  3192,\n",
       "  2542,\n",
       "  2605,\n",
       "  3731,\n",
       "  3728,\n",
       "  4312,\n",
       "  2889,\n",
       "  3768,\n",
       "  4279,\n",
       "  3266,\n",
       "  4270,\n",
       "  3128,\n",
       "  3574,\n",
       "  2585,\n",
       "  2936,\n",
       "  2601,\n",
       "  3705,\n",
       "  2760,\n",
       "  2882,\n",
       "  3735,\n",
       "  3743,\n",
       "  2593,\n",
       "  3506,\n",
       "  3324,\n",
       "  3661,\n",
       "  2556,\n",
       "  3722,\n",
       "  4189,\n",
       "  3607,\n",
       "  3729,\n",
       "  3738,\n",
       "  3616,\n",
       "  2856,\n",
       "  3833,\n",
       "  3629,\n",
       "  3308,\n",
       "  516,\n",
       "  4223,\n",
       "  2629,\n",
       "  3873,\n",
       "  3729,\n",
       "  3632,\n",
       "  4217,\n",
       "  3388,\n",
       "  2574,\n",
       "  3723,\n",
       "  3693,\n",
       "  3145,\n",
       "  4327,\n",
       "  3815,\n",
       "  3754,\n",
       "  4239,\n",
       "  2873,\n",
       "  2605,\n",
       "  4224,\n",
       "  527,\n",
       "  3694,\n",
       "  521,\n",
       "  523,\n",
       "  3732,\n",
       "  3729,\n",
       "  4293,\n",
       "  3621,\n",
       "  2922,\n",
       "  3954,\n",
       "  2542,\n",
       "  3767,\n",
       "  3731,\n",
       "  4223,\n",
       "  3308,\n",
       "  3192,\n",
       "  3232,\n",
       "  3693,\n",
       "  3729,\n",
       "  3738,\n",
       "  3616,\n",
       "  2856,\n",
       "  3833,\n",
       "  2968,\n",
       "  3774,\n",
       "  3744,\n",
       "  2653,\n",
       "  3190,\n",
       "  3729,\n",
       "  4217,\n",
       "  3674,\n",
       "  4224,\n",
       "  3728,\n",
       "  4271,\n",
       "  3621,\n",
       "  2889,\n",
       "  4232,\n",
       "  3417,\n",
       "  2856,\n",
       "  3400,\n",
       "  3099,\n",
       "  2585,\n",
       "  3248,\n",
       "  4260,\n",
       "  2778,\n",
       "  2579,\n",
       "  3729,\n",
       "  4222,\n",
       "  2760,\n",
       "  2922,\n",
       "  3614,\n",
       "  2873,\n",
       "  518,\n",
       "  3,\n",
       "  2,\n",
       "  3203,\n",
       "  3615,\n",
       "  3290,\n",
       "  2873,\n",
       "  4228,\n",
       "  2930,\n",
       "  3789,\n",
       "  3388,\n",
       "  2596,\n",
       "  2618,\n",
       "  3621,\n",
       "  2856,\n",
       "  2605,\n",
       "  3731,\n",
       "  3729,\n",
       "  3388,\n",
       "  3153,\n",
       "  3621,\n",
       "  3729,\n",
       "  3125,\n",
       "  2666,\n",
       "  2677,\n",
       "  3833,\n",
       "  4223,\n",
       "  2629,\n",
       "  2760,\n",
       "  4223,\n",
       "  2629,\n",
       "  3752,\n",
       "  3728,\n",
       "  3914,\n",
       "  3735,\n",
       "  3722,\n",
       "  3614,\n",
       "  3616,\n",
       "  2856,\n",
       "  3833,\n",
       "  3621,\n",
       "  2889,\n",
       "  4224,\n",
       "  2776,\n",
       "  3681,\n",
       "  3729,\n",
       "  3541,\n",
       "  3334,\n",
       "  3783,\n",
       "  3738,\n",
       "  2873,\n",
       "  518,\n",
       "  3,\n",
       "  2,\n",
       "  2605,\n",
       "  3731,\n",
       "  3722,\n",
       "  3632,\n",
       "  4217,\n",
       "  3388,\n",
       "  2574,\n",
       "  4223,\n",
       "  3407,\n",
       "  3375,\n",
       "  3569,\n",
       "  2864,\n",
       "  3042,\n",
       "  2873,\n",
       "  3127,\n",
       "  522,\n",
       "  3190,\n",
       "  3728,\n",
       "  3308,\n",
       "  3767,\n",
       "  3727,\n",
       "  4223,\n",
       "  3407,\n",
       "  3720,\n",
       "  3094,\n",
       "  3731,\n",
       "  4224,\n",
       "  2605,\n",
       "  3961,\n",
       "  3729,\n",
       "  3767,\n",
       "  3833,\n",
       "  3573,\n",
       "  3580,\n",
       "  2605,\n",
       "  516,\n",
       "  3192,\n",
       "  2942,\n",
       "  521,\n",
       "  520,\n",
       "  3902,\n",
       "  3092,\n",
       "  2760,\n",
       "  4223,\n",
       "  2629,\n",
       "  3873,\n",
       "  3621,\n",
       "  4223,\n",
       "  3407,\n",
       "  3833,\n",
       "  2922,\n",
       "  3728,\n",
       "  3605,\n",
       "  3083,\n",
       "  3687,\n",
       "  3723,\n",
       "  3729,\n",
       "  3713,\n",
       "  3094,\n",
       "  3398,\n",
       "  2882,\n",
       "  3674,\n",
       "  3931,\n",
       "  3723,\n",
       "  4224,\n",
       "  3242,\n",
       "  3738,\n",
       "  2873,\n",
       "  518,\n",
       "  3,\n",
       "  2,\n",
       "  2656,\n",
       "  3065,\n",
       "  2760,\n",
       "  4223,\n",
       "  2629,\n",
       "  3873,\n",
       "  3722,\n",
       "  2605,\n",
       "  3731,\n",
       "  3621,\n",
       "  2585,\n",
       "  3610,\n",
       "  3127,\n",
       "  3768,\n",
       "  4279,\n",
       "  3266,\n",
       "  4270,\n",
       "  3128,\n",
       "  3242,\n",
       "  2725,\n",
       "  3042,\n",
       "  2572,\n",
       "  2760,\n",
       "  4223,\n",
       "  3308,\n",
       "  3192,\n",
       "  3621,\n",
       "  2585,\n",
       "  3512,\n",
       "  3135,\n",
       "  2577,\n",
       "  3388,\n",
       "  3010,\n",
       "  2856,\n",
       "  3398,\n",
       "  2882,\n",
       "  3723,\n",
       "  3246,\n",
       "  3723,\n",
       "  2579,\n",
       "  3723,\n",
       "  2644,\n",
       "  3713,\n",
       "  4222,\n",
       "  3042,\n",
       "  2605,\n",
       "  3789,\n",
       "  3607,\n",
       "  4222,\n",
       "  2856,\n",
       "  3774,\n",
       "  2922,\n",
       "  3621,\n",
       "  2656,\n",
       "  3940,\n",
       "  2873,\n",
       "  518,\n",
       "  3,\n",
       "  2,\n",
       "  4223,\n",
       "  2629,\n",
       "  2542,\n",
       "  4227,\n",
       "  2705,\n",
       "  3206,\n",
       "  3776,\n",
       "  3128,\n",
       "  4232,\n",
       "  2596,\n",
       "  4222,\n",
       "  3083,\n",
       "  2856,\n",
       "  3728,\n",
       "  3833,\n",
       "  3128,\n",
       "  3290,\n",
       "  3729,\n",
       "  2856,\n",
       "  2889,\n",
       "  3508,\n",
       "  2559,\n",
       "  3283,\n",
       "  2629,\n",
       "  3388,\n",
       "  3621,\n",
       "  2585,\n",
       "  3914,\n",
       "  3735,\n",
       "  3723,\n",
       "  3833,\n",
       "  3685,\n",
       "  3774,\n",
       "  4284,\n",
       "  3729,\n",
       "  3569,\n",
       "  2867,\n",
       "  3468,\n",
       "  3614,\n",
       "  2873,\n",
       "  518,\n",
       "  3,\n",
       "  2,\n",
       "  3512,\n",
       "  3833,\n",
       "  3605,\n",
       "  4223,\n",
       "  2629,\n",
       "  3873,\n",
       "  3722,\n",
       "  3833,\n",
       "  2763,\n",
       "  2878,\n",
       "  3952,\n",
       "  3941,\n",
       "  3744,\n",
       "  3427,\n",
       "  4224,\n",
       "  3736,\n",
       "  3752,\n",
       "  3206,\n",
       "  3621,\n",
       "  3417,\n",
       "  3632,\n",
       "  4217,\n",
       "  3388,\n",
       "  2574,\n",
       "  3729,\n",
       "  3693,\n",
       "  3145,\n",
       "  4327,\n",
       "  4232,\n",
       "  2596,\n",
       "  2936,\n",
       "  2579,\n",
       "  3924,\n",
       "  3069,\n",
       "  3607,\n",
       "  2662,\n",
       "  4239,\n",
       "  2873,\n",
       "  2542,\n",
       "  4232,\n",
       "  2885,\n",
       "  2776,\n",
       "  3681,\n",
       "  3723,\n",
       "  3389,\n",
       "  3776,\n",
       "  4239,\n",
       "  2856,\n",
       "  2910,\n",
       "  516,\n",
       "  3584,\n",
       "  3941,\n",
       "  3605,\n",
       "  2998,\n",
       "  3728,\n",
       "  2922,\n",
       "  3094,\n",
       "  3744,\n",
       "  3427,\n",
       "  4224,\n",
       "  2579,\n",
       "  3731,\n",
       "  3833,\n",
       "  2653,\n",
       "  3190,\n",
       "  2937,\n",
       "  4217,\n",
       "  3674,\n",
       "  2542,\n",
       "  3738,\n",
       "  2873,\n",
       "  518,\n",
       "  3,\n",
       "  2,\n",
       "  2629,\n",
       "  3714,\n",
       "  2885,\n",
       "  2632,\n",
       "  3722,\n",
       "  3754,\n",
       "  3247,\n",
       "  3253,\n",
       "  3833,\n",
       "  2889,\n",
       "  3914,\n",
       "  3723,\n",
       "  3945,\n",
       "  3945,\n",
       "  4222,\n",
       "  2585,\n",
       "  3143,\n",
       "  3085,\n",
       "  4222,\n",
       "  2856,\n",
       "  3732,\n",
       "  2922,\n",
       "  3815,\n",
       "  3674,\n",
       "  4222,\n",
       "  3833,\n",
       "  3145,\n",
       "  3776,\n",
       "  2889,\n",
       "  3094,\n",
       "  2936,\n",
       "  3835,\n",
       "  3398,\n",
       "  2653,\n",
       "  3190,\n",
       "  3729,\n",
       "  3683,\n",
       "  3420,\n",
       "  3729,\n",
       "  3042,\n",
       "  2856,\n",
       "  3771,\n",
       "  3723,\n",
       "  3190,\n",
       "  3512,\n",
       "  4232,\n",
       "  3593,\n",
       "  4224,\n",
       "  2873,\n",
       "  518,\n",
       "  3],\n",
       " 'labels': [0, 1, 2],\n",
       " 'segs': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'clss': [0, 59, 134, 254, 290, 349, 439, 558, 609, 680, 739, 782, 856],\n",
       " 'src_txt': ['서 울   서 초 구   초 등 학 교   교 사   사 망 에   대 한   교 육 당 국   합 동 조 사   결 과 가   지 난   4 일   나 왔 지 만   새 롭 게   규 명 된   것 이   없 다 는   교 사 들 의   반 발 이   거 세 다 .',\n",
       "  '교 육 부 와   서 울 시 교 육 청 이   사 건   직 후   사 실 관 계   확 인 에   나 서 겠 다 며   합 동 조 사 에   착 수 했 지 만 ,   대 부 분   의 혹 을   경 찰   수 사 로   미 뤄 두 면 서   ‘ 용 두 사 미   조 사 ’ 가   되 고   말 았 다 는   것 이 다 .',\n",
       "  '전 국 초 등 교 사 노 조 는   “ ‘ 새 내 기   교 사 의   죽 음 에   무 거 운   책 임 감 을   느 낀 다 ’ 며   내 놓 은   결 과 라 고   납 득 할   수   없 을   정 도 로   허 술 하 다 .   합 동 조 사 단 이   해 야   할   일 은   해 당   학 교 가   낸   가 정 통 신 문   내 용 의   사 실   확 인 이   아 닌   고 인 의   업 무 상   고 충 을   면 면 히   공 개 하 는   것 이 어 야   했 다 ” 며   재 조 사 를   촉 구 했 다 .',\n",
       "  '전 국   교 사   4 만 명 은   지 난   주 말 에 도   철 저 한   진 상 규 명 을   촉 구 하 는   집 회 를   이 어 갔 다 .',\n",
       "  '실 제 로   합 동 조 사   결 과 의   대 부 분 은   학 교   쪽 이   고 인 의   사 망   직 후   냈 던   입 장 문 과   언 론 보 도 에 서   제 기 된   내 용 이   맞 는 지   확 인 하 는   수 준 에   그 쳤 다 .',\n",
       "  '올 해   3 월   이 후   고 인 의   학 급   담 임   교 체   사 실 이   없 었 다 거 나 ,   해 당   학 급 에 서   올 해   학 교 폭 력   신 고 가   없 었 으 며 ,   학 급   내   이 른 바   ‘ 연 필 사 건 ’   이 후   고 인 이   학 부 모 로 부 터   전 화 를   받 았 다 는   등 의   단 순   사 실   확 인 에   불 과 하 다 .',\n",
       "  '연 필 사 건   학 생 의   학 부 모 가   고 인 의   휴 대 전 화   번 호 를   알 게   된   경 위 나   담 임   자 격   시 비 와   같 은   폭 언 이   있 었 는 지   여 부 ,   학 교   쪽 이   연 필 사 건 을   원 만 히   중 재 했 다 고   한   7 월 1 3 일   이 후 에 도   추 가 적 인   학 부 모   민 원 이   있 었 는 지   등   정 작   규 명 이   필 요 한   의 혹 에   대 해 서 는   새 롭 게   밝 혀 낸   것 이   하 나 도   없 다 .',\n",
       "  '무 엇 보 다   합 동 조 사   결 과 에 는   고 인 이   사 망 에   이 르 기 까 지   학 교 나   학 교 장 의   책 임 은   없 었 는 지 에   대 한   내 용 이   쏙   빠 져   있 다 .',\n",
       "  '고 인 은   연 필 사 건   학 생 뿐   아 니 라   다 른   2 명 의   부 적 응   학 생 으 로   인 한   고 충 이   적 지   않 았 고 ,   모 두   1 0 차 례 나   학 교   쪽 에   학 생   지 도 의   어 려 움 을   이 유 로   상 담   요 청 을   한   바   있 다 .',\n",
       "  '그 러 나   학 교   쪽 은   고 인 에 게   얼 른   전 화 번 호 를   바 꾸 라 거 나   학 부 모 에 게   심 리 검 사   또 는   상 담 을   받 을   것 을   권 유 하 라 고   조 언 하 는   정 도 에   그 쳤 다 .',\n",
       "  '학 교 가   함 께   문 제 를   해 결 하 려 는   의 지 를   보 이 는   대 신   개 별   교 사 에 게   책 임 을   지 운   정 황 이   아 닐   수   없 다 .',\n",
       "  '심 지 어   학 교   쪽 은   지 난 달   최 초   작 성 한   입 장 문 에 서   연 필 사 건 이   원 만 히   해 결 된   것 처 럼   언 급 했 다 가   해 당   내 용 을   삭 제 했 는 데 ,   애 초   어 떤   의 도 로   작 성 한   것 인 지   규 명 될   필 요 가   있 다 .',\n",
       "  '교 육 당 국 은   재 발   방 지   대 책 을   촘 촘 하 게   마 련 하 는   일 도   중 요 하 지 만   제 대 로   된   진 상 규 명 이   우 선 이 라 는   점 을   명 심 해 야   한 다 .'],\n",
       " 'tgt_txt': None}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.6601198 1.2734241 1.1563554 1.0941479 1.0885135 1.0911074 1.096595\n",
      "  1.1044112 1.1063108 1.1072062 1.1194654 1.1539675 1.2159421]]\n",
      "[[ 0  1 12  2 11 10  9  8  7  6  3  5  4]]\n"
     ]
    }
   ],
   "source": [
    "result = summarize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1, 12,  2, 11, 10,  9,  8,  7,  6,  3,  5,  4], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['서울 서초구 초등학교 교사 사망에 대한 교육당국 합동조사 결과가 지난 4일 나왔지만 새롭게 규명된 것이 없다는 교사들의 반발이 거세다.',\n",
       " '교육부와 서울시교육청이 사건 직후 사실관계 확인에 나서겠다며 합동조사에 착수했지만, 대부분 의혹을 경찰 수사로 미뤄두면서 ‘용두사미 조사’가 되고 말았다는 것이다.',\n",
       " '교육당국은 재발 방지 대책을 촘촘하게 마련하는 일도 중요하지만 제대로 된 진상규명이 우선이라는 점을 명심해야 한다.',\n",
       " '전국초등교사노조는 “‘새내기 교사의 죽음에 무거운 책임감을 느낀다’며 내놓은 결과라고 납득할 수 없을 정도로 허술하다. 합동조사단이 해야 할 일은 해당 학교가 낸 가정통신문 내용의 사실 확인이 아닌 고인의 업무상 고충을 면면히 공개하는 것이어야 했다”며 재조사를 촉구했다.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[text[i] for i in result[0][:len(text)//3]]\n",
    "#[text[i] for i in result[0][:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 탐구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['BNK경남은행 간부의 562억원 횡령 사건이 채 잊히기도 전인데 또 다른 금융사고가 연발하고 있다.',\n",
    "'수법도 거래 기업의 미공개 정보로 주식을 거래해 부당이득을 취하고, 고객 몰래 계좌를 개설하는 등 눈앞의 이익을 위해 온갖 불법·편법 행위가 동원되고 있다.',\n",
    "'KB국민은행의 직원들이 고객사의 미공개정보를 이용해 주식을 사고팔아 127억원의 부당이득을 챙긴 사실이 최근 금융당국에 적발됐다.',\n",
    "'이들은 2021년 1월부터 올해 4월까지 61개 상장사의 증권 업무를 대행하며 알게 된 무상증자 규모와 일정을 주식 매수에 이용했다.',\n",
    "'정보 공개 전 미리 주식을 사뒀다가 공시 뒤 주가가 오르면 팔았다.',\n",
    "'무상증자를 하게 되면 기업재무구조가 건전한 것으로 풀이돼 주가에는 호재로 작용한다.',\n",
    "'이런 방식으로 66억원 정도를 챙겼고, 일부는 다른 부서의 동료나 가족 등에게도 정보를 전달했다.',\n",
    "'이 과정에서도 61억원 상당의 부당 이득이 발생했다.',\n",
    "'대구은행 일부 지점 직원 수십명은 평가 실적을 올리기 위해 지난해 1000여건이 넘는 고객 문서를 위조해 증권 계좌를 개설한 것으로 파악됐다.',\n",
    "'이 직원들은 내점한 고객을 상대로 증권사 연계 계좌를 만들어 달라고 요청한 뒤 해당 계좌 신청서를 복사해 고객의 동의 없이 같은 증권사의 계좌를 하나 더 만들었다.',\n",
    "'A증권사 위탁 계좌 개설 신청서를 받고, 같은 신청서를 복사해 A증권사 해외선물계좌까지 개설하는 방식이다.',\n",
    "'최근 한 고객이 동의하지 않은 계좌가 개설됐다는 사실을 알게 돼 대구은행에 민원을 제기하면서 직원들의 비리가 드러났다.',\n",
    "'대구은행은 문제를 인지하고도 금감원에 이 사실을 보고하지 않았고, 영업점들에 공문을 보내 불건전 영업행위를 예방하라고 안내하는 데 그쳤다.',\n",
    "'금융실명제법 위반, 사문서 위조 등에 해당할 수 있는 범죄행위를 대수롭지 않게 넘기는 안일함이 혀를 차게 한다.',\n",
    "'국내 은행은 땅 짚고 헤엄치기식 이자장사로 평균 1억원대 고연봉을 누리는 직종이다.',\n",
    "'시중은행은 미국발 고금리에 편승해 거둬들인 막대한 예대마진으로 최근 수년간 성과급 잔치를 벌여 국민의 눈총을 받았다.',\n",
    "'국민의 재산으로 손쉽게 수익을 올리는 직종이라면 누구보다 엄격한 도덕적 기준을 세워도 모자랄 판에 ‘내 몫을 더 챙기겠다’며 이기적 탐욕을 부리고 있으니 말문이 막힌다.',\n",
    "'자체 내부통제가 안 된다면 현행 솜방망이 처벌 수위를 높이는 수밖에 없다.',\n",
    "'주요 동기가 경제적 이익인 만큼 벌금이나 과징금, 양형 부과 수준을 크게 높여 법 무서운 줄 알도록 해야 한다.',\n",
    "'주요 선진국에서 이미 도입한 불공정거래 범죄자에 대한 자본시장 거래제한제도도 적극 검토할 필요가 있다.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss\n",
    "\n",
    "array = []\n",
    "text = kss.split_sentences(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = []\n",
    "src.append(text)\n",
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_src_txt = [''.join(s) for s in text]\n",
    "original_src_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(original_src_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = [i for i, s in enumerate(text) if (len(s) > 1)]\n",
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [text[i][:2000] for i in idxs]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[:1000]\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_txt = [''.join(sent) for sent in text]\n",
    "src_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' [SEP] [CLS] '.join(src_txt)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_subtokens = tokenizer.tokenize(text)\n",
    "src_subtokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "print(vocab.get('▁('))\n",
    "print([tokenizer.encode(token) for token in src_subtokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_subtokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_subtokens = src_subtokens[:510]  ## 512가 최대인데 [SEP], [CLS] 2개 때문에 510\n",
    "len(src_subtokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_subtokens = ['[CLS]'] + src_subtokens + ['[SEP]']\n",
    "len(src_subtokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_subtoken_idxs = tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "src_subtoken_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text) # 원문 문장 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_subtoken_idxs.count(2)  # 2는 [CLS] 토큰\n",
    "# 즉 9개의 문장에 해당하는 [CLS] 토큰만 입력된다는 이야기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " self.sep_vid = self.tokenizer.convert_tokens_to_ids(self.sep_token)\n",
    "\n",
    "_segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
    "segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "segments_ids = []\n",
    "for i, s in enumerate(segs):\n",
    "    if (i % 2 == 0):\n",
    "        segments_ids += s * [0]\n",
    "    else:\n",
    "        segments_ids += s * [1]\n",
    "cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "일본 정부가 문재인 대통령이 도쿄올림픽을 계기로 일본을 방문하는 방향으로 한일 양국이 조율하고 있다는 15일 요미우리신문의 보도를 부인했다.\n",
    "\n",
    "정부 대변인인 가토 가쓰노부(加藤勝信) 관방장관은 이날 오전 정례 기자회견에서 관련 질문에 “말씀하신 보도와 같은 사실이 없는 것으로 안다”고 밝혔다.\n",
    "\n",
    "앞서 요미우리는 한국 측이 도쿄올림픽을 계기로 한 문 대통령의 방일을 타진했고, 일본 측은 수용하는 방향이라고 이날 보도했다.\n",
    "\n",
    "한국 측은 문 대통령의 방일 때 스가 요시히데(菅義偉) 총리와 처음으로 정상회담을 하겠다는 생각이라고 요미우리는 전했다.\n",
    "\n",
    "가토 장관은 한일 정상회담에 대한 일본 정부의 자세에 대해 “그런 사실이 없기 때문에 가정의 질문에 대해 답하는 것을 삼가겠다”고 말했다.\n",
    "\n",
    "그는 한국 측의 독도방어훈련에 ‘어떤 대항 조치를 생각하고 있느냐’는 질문에는 “한국 해군의 훈련에 대해 정부로서는 강한 관심을 가지고 주시하는 상황이어서 지금 시점에선 논평을 삼가겠다”고 말을 아꼈다.\n",
    "\n",
    "가토 장관은 “다케시마(竹島·일본이 주장하는 독도의 명칭)는 역사적 사실에 비춰봐도, 국제법상으로도 명백한 일본 고유의 영토”라며 독도 영유권 주장을 되풀이했다.\n",
    "\n",
    "그러면서 “다케시마 문제에 대해서는 계속 우리나라의 영토, 영해, 영공을 단호히 지키겠다는 결의로 냉정하고 의연하게 대응해갈 생각”이라고 밝혔다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(filter(None, text.split('\\n')))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepro.tokenization_kobert import KoBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertData():\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\", do_lower_case=True)\n",
    "\n",
    "        self.sep_token = '[SEP]'\n",
    "        self.cls_token = '[CLS]'\n",
    "        self.pad_token = '[PAD]'\n",
    "        # self.sep_vid = self.tokenizer.token2idx[self.sep_token]\n",
    "        # self.cls_vid = self.tokenizer.token2idx[self.cls_token]\n",
    "        # self.pad_vid = self.tokenizer.token2idx[self.pad_token]\n",
    "\n",
    "        self.sep_vid = self.tokenizer.convert_tokens_to_ids(self.sep_token)\n",
    "        self.cls_vid = self.tokenizer.convert_tokens_to_ids(self.cls_token)\n",
    "        self.pad_vid = self.tokenizer.convert_tokens_to_ids(self.pad_token)\n",
    "\n",
    "    def preprocess(self, src):\n",
    "\n",
    "        if (len(src) == 0):\n",
    "            return None\n",
    "\n",
    "        original_src_txt = [' '.join(s) for s in src]\n",
    "\n",
    "        labels = [0] * len(src)\n",
    "        # for l in oracle_ids:\n",
    "        #     labels[l] = 1\n",
    "\n",
    "        idxs = [i for i, s in enumerate(src) if (len(s) > 1)]\n",
    "\n",
    "        src = [src[i][:1000] for i in idxs]\n",
    "        #labels = [labels[i] for i in idxs]\n",
    "        src = src[:2000]\n",
    "        #labels = labels[:self.args.max_nsents]\n",
    "\n",
    "        # if (len(src) < 1):\n",
    "        #     return None\n",
    "        # if (len(labels) == 0):\n",
    "        #     return None\n",
    "\n",
    "        src_txt = [' '.join(sent) for sent in src]\n",
    "        # text = [' '.join(ex['src_txt'][i].split()[:self.args.max_src_ntokens]) for i in idxs]\n",
    "        # text = [_clean(t) for t in text]\n",
    "        text = ' [SEP] [CLS] '.join(src_txt)\n",
    "        src_subtokens = self.tokenizer.tokenize(text)\n",
    "        src_subtokens = src_subtokens[:510]\n",
    "        src_subtokens = ['[CLS]'] + src_subtokens + ['[SEP]']\n",
    "\n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
    "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "        segments_ids = []\n",
    "        for i, s in enumerate(segs):\n",
    "            if (i % 2 == 0):\n",
    "                segments_ids += s * [0]\n",
    "            else:\n",
    "                segments_ids += s * [1]\n",
    "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
    "        #labels = labels[:len(cls_ids)]\n",
    "\n",
    "        #tgt_txt = '<q>'.join([' '.join(tt) for tt in tgt])\n",
    "        src_txt = [original_src_txt[i] for i in idxs]\n",
    "        return src_subtoken_idxs, segments_ids, cls_ids, src_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt2input(text):\n",
    "    data = list(filter(None, text.split('\\n')))\n",
    "    #data = split_sentences(text)\n",
    "    bertdata = BertData()\n",
    "    txt_data = bertdata.preprocess(data)\n",
    "    data_dict = {\"src\":txt_data[0],\n",
    "                \"segs\":txt_data[1],\n",
    "                \"clss\":txt_data[2],\n",
    "                \"src_txt\":txt_data[3]}\n",
    "    input_data = []\n",
    "    input_data.append(data_dict)\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = txt2input(text)\n",
    "input_data[0]['src'].count(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = txt2input(text)\n",
    "len(inputs[0]['src']), len(inputs[0]['clss']), len(inputs[0]['segs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "def _pad(data, pad_id, width=-1):\n",
    "    if (width == -1):\n",
    "        width = max(len(d) for d in data)\n",
    "    rtn_data = [d + [pad_id] * (width - len(d)) for d in data]\n",
    "    return rtn_data\n",
    "\n",
    "pre_src = [x['src'] for x in inputs]\n",
    "pre_segs = [x['segs'] for x in inputs]\n",
    "pre_clss = [x['clss'] for x in inputs]\n",
    "\n",
    "src = torch.tensor(_pad(pre_src, 0))\n",
    "segs = torch.tensor(_pad(pre_segs, 0))\n",
    "mask_src = ~(src == 0)\n",
    "\n",
    "clss = torch.tensor(_pad(pre_clss, -1))\n",
    "mask_cls = ~(clss == -1)\n",
    "clss[clss == -1] = 0\n",
    "\n",
    "clss.to(device).long()\n",
    "mask_cls.to(device).long()\n",
    "segs.to(device).long()\n",
    "mask_src.to(device).long()\n",
    "\n",
    "# checkpoint = torch.load(\"D:/KoBertSum/ext/models/model_step_26000.pt\")\n",
    "# model = ExtSummarizer(args, device, checkpoint)\n",
    "# model.eval()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     sent_scores, mask = model(src, segs, clss, mask_src, mask_cls)\n",
    "#     print(sent_scores)\n",
    "#     sent_scores = sent_scores + mask.float()\n",
    "#     sent_scores = sent_scores.cpu().data.numpy()\n",
    "#     print(sent_scores)\n",
    "#     selected_ids = np.argsort(-sent_scores, 1)\n",
    "#     print(selected_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(\"monologg/kobert\")\n",
    "top_vec = model(input_ids=src, token_type_ids=segs, attention_mask=mask_src)[0]\n",
    "top_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src.size(), clss.size(), segs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_vec[torch.arange(top_vec.size(0)).unsqueeze(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
    "sents_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
    "sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
    "sent_scores = self.ext_layer(sents_vec, mask_cls).squeeze(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mecab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
